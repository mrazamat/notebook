# Языковые модели и N-граммы: Простое введение

**Курс:** Алгоритмы NLP  
**Специальность:** Компьютерная лингвистика  
**Уровень сложности:** Начинающий



## Оглавление

1. [Что такое языковые модели?](#что-такое-языковые-модели)
2. [N-граммовые модели](#n-граммовые-модели)
3. [Как работают вероятности](#как-работают-вероятности)
4. [Показатель Perplexity](#показатель-perplexity)
5. [Примеры на русском языке](#примеры-на-русском-языке)
6. [Простые упражнения](#простые-упражнения)
7. [Вопросы для проверки знаний](#вопросы-для-проверки-знаний)



## Что такое языковые модели?

### Простое объяснение

**Языковая модель** — это программа, которая учится предсказывать, какое слово будет следующим в предложении.

Например:
- Если вы начнёте писать "Большая..." → модель предложит "кошка" или "собака"
- Если вы напишете "Москва — столица..." → модель предложит "России"

### Как это работает?

Языковая модель учится на большом количестве текстов и запоминает:
- Какие слова часто идут после других слов
- Какие комбинации слов встречаются в языке
- Какие комбинации редкие или невозможные

### Зачем это нужно?

- Автодополнение в телефонах и браузерах
- Исправление опечаток
- Перевод текстов
- Общение с чат-ботами
- Проверка качества текста



## N-граммовые модели

### Что такое N-грамм?

**N-грамм** — это последовательность из N слов, которые идут подряд.

### Примеры на русском

**Текст:** "кот сидит на окне и смотрит на птиц"

#### Слова (1-граммы):
```
кот
сидит
на
окне
и
смотрит
на
птиц
```

#### Пары слов (биграммы, 2-граммы):
```
кот сидит
сидит на
на окне
окне и
и смотрит
смотрит на
на птиц
```

#### Тройки слов (триграммы, 3-граммы):
```
кот сидит на
сидит на окне
на окне и
окне и смотрит
и смотрит на
смотрит на птиц
```

### Типы N-грамм

| Название | N | Пример | Сложность |
|-||--|--|
| Унигра́мма | 1 | "кот" | Очень простая |
| Биграмма | 2 | "кот сидит" | Простая |
| Триграмма | 3 | "кот сидит на" | Средняя |
| 4-грамма | 4 | "кот сидит на окне" | Сложная |

### Правило: Чем больше N, тем...

✅ **Хорошо:**
- Модель лучше понимает смысл
- Результаты более реалистичны

❌ **Плохо:**
- Нужно больше данных для обучения
- Сложнее найти одинаковые N-граммы в тексте
- Требуется больше памяти в компьютере



## Как работают вероятности

### Простой пример

**Обучающий текст:**
```
"кот сидит. кот спит. собака лает."
```

### Шаг 1: Подсчет биграмм

Сколько раз встречается каждая пара слов?

```
кот сидит    → 1 раз
кот спит     → 1 раз
собака лает  → 1 раз
(точка) кот  → 2 раза
```

### Шаг 2: Вычисление вероятности

**Вероятность** — это число от 0 до 1, показывающее, насколько вероятно событие.

Формула для биграммы:

$$P(\text{слово}_2 | \text{слово}_1) = \frac{\text{Сколько раз встречается пара}}{\text{Сколько раз встречается первое слово}}$$

### Пример вычисления

**Вопрос:** Какова вероятность слова "сидит" после слова "кот"?

**Решение:**

```
Слово "кот" встречается 2 раза (кот сидит, кот спит)
Пара "кот сидит" встречается 1 раз

P(сидит | кот) = 1 / 2 = 0.5 = 50%
```

**Интерпретация:** После слова "кот" слово "сидит" встречается в 50% случаев.

### Другие примеры

```
Слово "кот" встречается 2 раза
Пара "кот спит" встречается 1 раз
P(спит | кот) = 1 / 2 = 0.5 = 50%

Слово "собака" встречается 1 раз
Пара "собака лает" встречается 1 раз
P(лает | собака) = 1 / 1 = 1.0 = 100%
```

### Что значат разные вероятности?

| Вероятность | Значение |
|-|-|
| 0 или 0% | Никогда не встречается |
| 0.25 или 25% | Редкое явление |
| 0.5 или 50% | Встречается часто |
| 1.0 или 100% | Всегда встречается |



## Показатель Perplexity

### Что такое Perplexity?

**Perplexity (перплексность)** — это число, которое показывает, насколько хорошо модель предсказывает текст.

**Простое объяснение:** Perplexity показывает, во скольких словах в среднем "не уверена" модель.

### Примеры

| Перплексность | Качество | Пример |
||||
| 2 | Отлично | Модель очень уверена, выбирает из 2 вариантов |
| 10 | Хорошо | Модель довольно уверена, выбирает из 10 вариантов |
| 100 | Плохо | Модель не уверена, выбирает из 100 вариантов |
| 1000 | Очень плохо | Модель совсем не уверена |

### Главное правило

**Низкая перплексность = хорошая модель ✅**
**Высокая перплексность = плохая модель ❌**

### Простое объяснение через игру

Представьте, что вы играете в словесную игру:

**Сценарий 1 (низкая перплексность = 5):**
- Вам дают контекст: "Кот сидит на..."
- Вы должны угадать следующее слово
- Модель уверена, что это может быть: окне, кровати, столе, диване, коврике
- Всего 5 вариантов → Перплексность ≈ 5

**Сценарий 2 (высокая перплексность = 100):**
- Вам дают контекст: "В магазине..."
- Модель неуверена, может быть 100 разных слов
- Продавец, фрукты, музыка, люди, товары и т.д.
- Всего 100 вариантов → Перплексность ≈ 100

### Как вычислить Perplexity (для любопытных)

Если у вас есть вероятность последовательности слов, то:

$$\text{Perplexity} = \sqrt[n]{\frac{1}{\text{Вероятность}}}$$

где n — количество слов

**Конкретный пример:**

```
Тестовая фраза: "кот сидит"
P(кот) = 0.4
P(сидит | кот) = 0.6
Общая вероятность = 0.4 × 0.6 = 0.24

Perplexity = sqrt(1 / 0.24) = sqrt(4.17) ≈ 2.0
```



## Примеры на русском языке

### Пример 1: Модель реального текста

**Обучающий текст (маленький корпус):**
```
"Я люблю программирование. Программирование это искусство.
Я пишу код каждый день. Код работает хорошо."
```

### Что запомнила модель

**Биграммы и их вероятности:**

```
P(люблю | Я) = 1/1 = 1.0 (100%)
  → После "Я" всегда идет "люблю"

P(программирование | Я) = не встречается = 0%

P(программирование | люблю) = 1/1 = 1.0 (100%)
  → После "люблю" идет "программирование"

P(это | программирование) = 1/2 = 0.5 (50%)
  → Из двух "программирование", в одном идет "это"

P(искусство | это) = 1/1 = 1.0 (100%)
P(пишу | Я) = 1/1 = 1.0 (100%)
P(код | пишу) = 1/1 = 1.0 (100%)
```

### Что может предсказать модель?

Начало: "Я"
- Модель предложит: "люблю" (100% вероятность)

Начало: "люблю"
- Модель предложит: "программирование" (100% вероятность)

Начало: "программирование"
- Модель может предложить:
  - "это" (50% вероятность)
  - Или другое слово (50% вероятность)

### Пример 2: Простое вычисление Perplexity

**Тестовая фраза:** "Я люблю программирование"

**Вероятности:**
```
P(Я в начале) = 0.25 (произвольно)
P(люблю | Я) = 1.0
P(программирование | люблю) = 1.0

Общая вероятность = 0.25 × 1.0 × 1.0 = 0.25
```

**Расчет Perplexity:**
```
Перплексность = (1 / 0.25)^(1/3) = 4^(1/3) ≈ 1.59

Интерпретация: Модель выбирает в среднем из ~1.6 вариантов
Это означает, что модель очень уверена в этом тексте!
```

### Пример 3: Проблема неизвестных слов

**Обучающий текст:**
```
"кот сидит. кот спит. кот прыгает."
```

**Неизвестное слово в тесте:** "собака"

**Проблема:**
```
P(собака | кот) = 0 / 3 = 0

Если вероятность = 0, то Perplexity = бесконечность!
```

**Решение:** Добавить маленькую вероятность всем словам, даже неизвестным

```
С correction:
P(собака | кот) = 0.01 (вместо 0)

Теперь можно вычислить Perplexity нормально
```



## Простые упражнения

### Упражнение 1: Подсчет биграмм

**Задача:** Разделите текст на биграммы

**Текст:**
```
"Зима наступила. Зима холодная. Люблю зиму."
```

**Требуется:** Перечислите все биграммы




### Упражнение 2: Вычисление вероятностей

**Задача:** Используя текст из Упражнения 1, вычислите вероятности

**Текст:**
```
"Зима наступила. Зима холодная. Люблю зиму."
```

**Требуется:** Вычислите:
- a) P(наступила | Зима)
- b) P(холодная | Зима)
- c) P(зиму | Люблю)

**Подсказка:**

Сначала посчитайте, сколько раз встречается первое слово, потом — сколько раз встречается вся пара.



### Упражнение 3: Какая модель лучше?

**Задача:** Сравните две модели

**Модель A:** Perplexity = 15
**Модель B:** Perplexity = 25

**Требуется:** 
- Какая модель лучше?
- Почему?
- Что это означает в простых словах?

**Решение:**

```
Модель A лучше, потому что у неё МЕНЬШЕ перплексность.

Это означает, что Модель A более уверена в своих предсказаниях.
Модель A выбирает в среднем из 15 вариантов,
а Модель B — из 25 вариантов.
```



### Упражнение 4: Практическое задание

**Задача:** Создайте свою маленькую языковую модель

1. **Напишите текст из 3-4 предложений на русском** (или скопируйте):

```
Текст:
_______________________________________
_______________________________________
_______________________________________
```

2. **Выпишите все биграммы:**

```
1. ___________
2. ___________
3. ___________
...
```

3. **Выберите слово X** (которое встречается в тексте несколько раз)

4. **Посчитайте:** P(Y | X) для разных слов Y, которые идут после X

```
P(_ | X) = ___
P(_ | X) = ___
P(_ | X) = ___
```



### Упражнение 5: Интерпретация Perplexity

**Задача:** Объясните результаты

Представьте, что вы обучили модель на текстах о природе и получили:

```
На текстах про природу:     Perplexity = 8
На текстах про спорт:       Perplexity = 50
На случайном текстовом супе: Perplexity = 200
```

**Требуется:**
- Почему разные тексты дают разную перплексность?
- Какой текст модель предсказывает лучше всего?





## Вопросы для проверки знаний

### Легкие вопросы

1. **Что такое N-грамм?**
   - A) Большой текст
   - B) Последовательность N слов подряд
   - C) Ошибка в тексте
   - D) Словарь слов

2. **Какой N-грамм проще: биграмма или триграмма?**

3. **Что означает биграмма?**
   - A) 1 слово
   - B) 2 слова подряд
   - C) 3 слова подряд
   - D) Много слов

4. **Какая перплексность лучше: 10 или 100?**

5. **Какой текст из биграммы "кот сидит" является контекстом?**
   - A) "кот"
   - B) "сидит"
   - C) "кот сидит"

6. **Если P(сидит | кот) = 0.8, что это означает?**
   - A) После "кот" всегда идет "сидит"
   - B) После "кот" в 80% случаев идет "сидит"
   - C) После "кот" никогда не идет "сидит"
   - D) "сидит" встречается в 0.8% текста

7. **Что такое языковая модель?**

8. **Для чего нужны N-граммовые модели?**

### Средние вопросы

9. **Почему триграмма дает лучше результаты, чем биграмма?**

10. **В чем минус очень больших N-грамм (например, 10-граммы)?**

11. **Что такое Perplexity простыми словами?**

12. **Если у вас есть текст "Я люблю кошек", сколько биграмм вы можете составить?**
    - A) 1
    - B) 2
    - C) 3
    - D) 4

13. **Какую информацию теряет биграмма "большой дом" по сравнению с триграммой "очень большой дом"?**

14. **Если текст в тестовом наборе очень похож на обучающий текст, перплексность будет низкой или высокой?**

15. **Что означает P(слово) = 0?**

### Сложные вопросы

16. **Почему перплексность разных моделей не всегда сравнима?**

17. **Что произойдет с перплексностью, если в тестовом тексте появится слово, которого не было в обучающих данных?**

18. **Может ли модель, обученная на одном языке, хорошо работать на другом языке?**

19. **Какой вид текста (новости, литература, техдокументация) может быть сложнее для языковой модели и почему?**

20. **Как вы думаете, какой N-грамм был бы лучше для предсказания следующего слова в вашем сообщении в чате?**



## Основные термины (глоссарий)

| Термин | Объяснение |
|--|--|
| **N-грамм** | Последовательность из N слов подряд |
| **Биграмма** | 2 слова подряд |
| **Триграмма** | 3 слова подряд |
| **Унигра́мма** | 1 слово |
| **Вероятность** | Число от 0 до 1, показывающее вероятность события |
| **Языковая модель** | Программа, которая учится предсказывать следующее слово |
| **Perplexity** | Показатель качества модели (чем меньше, тем лучше) |
| **Контекст** | Слова, которые идут перед интересующим нас словом |
| **Корпус** | Большое собрание текстов для обучения |
| **Обучающие данные** | Текст, на котором модель учится |
| **Тестовые данные** | Текст, на котором мы проверяем модель |



## Запомните главное

### 3 основные идеи

1. **N-граммы** — это способ смотреть на язык как на последовательности слов
   - Биграмма смотрит на 2 слова
   - Триграмма смотрит на 3 слова
   - Больше слов → больше контекста, но меньше примеров

2. **Вероятность** — это как часто встречается слово в контексте
   - P(слово | контекст) = сколько раз встретилось / сколько раз встретился контекст
   - Это число от 0 до 1

3. **Perplexity** — это качество модели
   - Низкая перплексность = хорошая модель
   - Высокая перплексность = плохая модель
   - Perplexity = среднее количество вариантов, из которых выбирает модель



## Проверка понимания (итоговый тест)

**Ответьте на эти вопросы, чтобы убедиться, что вы поняли материал:**

1. Объясните, что такое биграмма, в одном предложении

2. Как вычислить вероятность одного слова после другого? (Напишите формулу словами)

3. Какая перплексность лучше: 50 или 5? Почему?

4. Придумайте пример биграммы на русском языке

5. Почему триграмма не всегда лучше биграммы?



## Дополнительные материалы

### Визуализация биграмм

```
Текст: "Кот прыгает на кровать"

биграммы визуально:
┌─────────────────┐
│ Кот прыгает     │
│   ↓ ↓           │
│ прыгает на      │
│    ↓ ↓          │
│ на кровать      │
└─────────────────┘
```

### Как это работает на практике

**Вы пишете в поисковик:** "когда..."

**Модель предлагает:**
- когда проснуться (вероятность 30%)
- когда начинается (вероятность 25%)
- когда деньги приходят (вероятность 20%)
- когда откроется магазин (вероятность 15%)
- другое (вероятность 10%)

**Perplexity ≈ 4**, значит, модель выбирает в среднем из 4 вариантов



## Заключение

Теперь вы знаете:

✅ Что такое N-граммовые модели
✅ Как вычисляются вероятности
✅ Что такое Perplexity и почему она важна
✅ Как это применяется в реальной жизни

**Следующие шаги:**
- Попробуйте написать простую N-граммовую модель
- Экспериментируйте с разными текстами
- Посмотрите, как это работает в реальных приложениях



**Спасибо за внимание!**


