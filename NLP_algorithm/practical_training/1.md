# Введение в алгоритмы обработки естественного языка (NLP)
## Основные понятия и история научного развития

### 1. Основные понятия NLP

**Обработка естественного языка (Natural Language Processing, NLP)** — междисциплинарная область на стыке компьютерной лингвистики, искусственного интеллекта и информатики, занимающаяся разработкой алгоритмов и моделей для автоматического анализа, понимания и генерации человеческого языка.

**Ключевые задачи NLP:**
- **Токенизация** — разбиение текста на слова, предложения или другие значимые единицы (токены).
- **Стемминг и лемматизация** — приведение слов к их нормальной форме.
- **Определение части речи (POS-tagging)** — маркировка слов соответствующими частями речи.
- **Синтаксический анализ (Parsing)** — анализ грамматической структуры предложения.
- **Распознавание именованных сущностей (NER)** — выделение имен собственных (имена, организации, даты).
- **Анализ тональности (Sentiment Analysis)** — определение эмоциональной окраски текста.
- **Машинный перевод (Machine Translation)** — автоматический перевод с одного языка на другой.
- **Вопросно-ответные системы (QA Systems)** — автоматическое формирование ответов на вопросы пользователя.
- **Генерация текста (Text Generation)** — создание связного текста на заданную тему.

**Основные уровни лингвистического анализа:**
- Фонологический
- Морфологический
- Синтаксический
- Семантический
- Прагматический

### 2. История научного развития NLP

**Периодизация развития NLP:**

#### 2.1. Ранний период (1950-е – 1960-е годы)
- **1950:** Тест Тьюринга — первая концептуальная основа для оценки "интеллектуальности" машины.
- **1954:** Эксперимент Джорджтаунского университета — машинный перевод 60 русских предложений на английский с использованием словарных замен.
- **1966:** Отчет ALPAC — критика состояния машинного перевода, приведшая к сокращению финансирования и "зиме" в развитии NLP.

#### 2.2. Символьные системы и лингвистические правила (1970-е – 1980-е годы)
- Разработка экспертных систем и грамматик (например, **ELIZA** — первый чат-бот, **SHRDLU** — система для работы с виртуальными объектами).
- **Теория:** Усиление роли лингвистических знаний (трансформационные грамматики Хомского).
- Системы, основанные на правилах (rule-based systems), требующие ручного кодирования лингвистических знаний.

#### 2.3. Статистические методы и машинное обучение (1990-е – 2000-е годы)
- **"Статистическая революция"** — переход от правил к вероятностным моделям.
- Использование скрытых марковских моделей (HMM) для POS-теггинга и NER.
- Модели на основе n-грамм для языкового моделирования.
- Развитие машинного обучения (SVM, логистическая регрессия) для классификации текстов.
- **Ключевое событие:** Рост вычислительных мощностей и доступности больших текстовых корпусов.

#### 2.4. Глубокое обучение и современный период (2010-е – настоящее время)
- **Поворотный момент:** Успехи в использовании рекуррентных нейронных сетей (RNN, LSTM) для моделирования последовательностей.
- **2013-2014:** Внедрение word2vec и других методов распределенных представлений слов (эмбеддингов).
- **2017:** Появление архитектуры **Transformer** (статья "Attention is All You Need") — фундамент для современных больших языковых моделей.
- **2018-2024:** Эра больших языковых моделей (LLM) — BERT, GPT-3/4, Claude и другие, основанные на трансформерах, показывающие революционные результаты в генерации и понимании текста.

**Текущие тренды:**
- Мультимодальность (обработка текста, изображений, звука вместе)
- Создание эффективных (меньших по размеру) и энергоэффективных моделей
- Повышение объяснимости (XAI) и снижение смещений (bias) в моделях
- Развитие few-shot и zero-shot обучения

### 3. Практические упражнения

**Упражнение 1: Базовый анализ текста**
#### Задание: напишите код на Python с использованием библиотеки nltk или spaCy для:
 1. Токенизации предложения "Обработка естественного языка открывает новые возможности для человеко-машинного взаимодействия."
 2. Выполнения стемминга и лемматизации каждого токена.
 3. Определения частей речи (POS) для каждого слова.

* **Подсказка**: установите библиотеки: pip install nltk spacy
* Для spaCy загрузите модель: python -m spacy download ru_core_news_sm

### Упражнение 2: Сравнение подходов

* Изучите фрагмент кода системы на правилах (например, простой чат-бот с ключевыми словами) и системы на статистике (классификатор тональности на логистической регрессии).
* Объясните, в чем принципиальная разница в подходах и каковы ограничения каждого.

### 4. Вопросы для самопроверки
1. **Определение**: Дайте определение NLP и назовите три ключевые задачи, отличающие его от простого анализа текста.
2. **История**: Каковы были основные причины "зимы" в развитии NLP в конце 1960-х годов? Какой отчет стал ее катализатором?
3. **Методы**: В чем принципиальное различие между символическими (rule-based) и статистическими подходами в NLP? Приведите по одному примеру задачи, для которой каждый подход лучше подходит.
4. **Архитектуры**: Почему архитектура Transformer стала прорывом по сравнению с рекуррентными нейронными сетями (RNN/LSTM)? Назовите два ключевых преимущества.
5. **Этика**: Какие этические проблемы возникают с развитием больших языковых моделей (LLM)? Назовите не менее трех.

### 5. Дополнительные ресурсы
* Книги: Jurafsky & Martin "Speech and Language Processing" (3rd ed.)
* Курсы: Stanford CS224N "Natural Language Processing with Deep Learning"
* Библиотеки: NLTK, spaCy, Hugging Face Transformers, PyTorch, TensorFlow
* Датасеты: GLUE, SuperGLUE, SQuAD, Russian NLP datasets (RuBERT)

