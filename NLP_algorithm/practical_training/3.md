# Токены и нормализация текста: Роль и проблемы в компьютерной лингвистике

## Оглавление
1. [Введение](#введение)
2. [Фундаментальные концепции](#фундаментальные-концепции)
3. [Токенизация](#токенизация)
4. [Нормализация текста](#нормализация-текста)
5. [Проблемы и вызовы](#проблемы-и-вызовы)
6. [Практические упражнения](#практические-упражнения)
7. [Контрольные вопросы](#контрольные-вопросы)

---

## Введение

Обработка естественного языка (NLP) начинается с преобразования сырого текста в структурированные данные, которые могут быть использованы алгоритмами машинного обучения. Два ключевых этапа этого процесса — **токенизация** и **нормализация текста** — являются критическими компонентами любого NLP конвейера.

Без правильной токенизации и нормализации даже самые сложные модели машинного обучения будут работать неэффективно. Эта лекция исследует роль этих процессов, описывает различные подходы и обсуждает практические проблемы, которые возникают при их реализации на реальных языках, особенно на русском языке.

---

## Фундаментальные концепции

### Что такое токен?

**Токен** — это минимальная единица текста, на которую разбивается исходный текст для дальнейшей обработки. Это может быть:

- **Слово**: "кот", "быстро", "прекрасный"
- **Подслово (подсловие)**: "быстр", "но", используется в BPE и SentencePiece
- **Символ**: отдельные буквы или знаки
- **N-грамма**: последовательность из N токенов

### Что такое нормализация?

**Нормализация** — это процесс приведения текста к стандартной форме, целью которого является:

- Снижение разреженности данных
- Унификация представления информации
- Улучшение качества работы моделей
- Сокращение размера словаря

Нормализация может включать преобразование в нижний регистр, удаление диакритических знаков, стемизацию, лемматизацию и другие операции.

---

## Токенизация

### 1. Уровни токенизации

#### 1.1 Токенизация на уровне слов

Разбиение текста на слова на основе пробелов и пунктуации.

**Пример:**
```
Входной текст: "Привет, мир! Как дела?"
Токены: ["Привет", ",", "мир", "!", "Как", "дела", "?"]
```

**Преимущества:**
- Простота реализации
- Интуитивность
- Быстрая обработка

**Недостатки:**
- Проблемы с пунктуацией и специальными символами
- Неправильная обработка сокращений ("U.S.A" → ["U", ".", "S", ".", "A"])
- Сложности с многословными выражениями ("New York" → ["New", "York"])
- Неэффективна для других языков (китайский, японский)

#### 1.2 Токенизация на уровне подслов (Subword Tokenization)

Разбиение текста на подслова, позволяющее работать с неизвестными и редкими словами.

**Основные методы:**

**а) Byte Pair Encoding (BPE)**
- Итеративно объединяет наиболее частые пары символов
- Используется в GPT-2, RoBERTa
- Позволяет кодировать любой текст (fallback к символам)

**Алгоритм:**
1. Начинаем с набора всех отдельных символов
2. Подсчитываем частоту всех пар символов
3. Объединяем наиболее частую пару в новый токен
4. Повторяем шаги 2-3 до достижения желаемого размера словаря

**б) WordPiece**
- Разработана Google для BERT
- Похожа на BPE, но использует вероятность вместо частоты
- Начинает с целых слов, затем разбивает редкие слова

**в) SentencePiece**
- Обрабатывает текст как последовательность символов
- Работает с любым языком без предварительной токенизации
- Использует алгоритм BPE или Unigram LM

**г) Unigram Language Model**
- Выбирает подслова, которые минимизируют потери кросс-энтропии
- Использует вероятностный подход

**Пример BPE:**
```
Исходный текст: "low low lowest low"
Исходные токены: l o w l o w l o w e s t l o w

Итерация 1: l и o наиболее частая пара → объединяем в lo
lo w lo w lo w e s t lo w

Итерация 2: lo и w наиболее частая пара → объединяем в low
low low low e s t low
```

#### 1.3 Специализированная токенизация

**Для русского языка:**
- Учет сложных морфологических структур
- Обработка приставок и суффиксов
- Работа с падежами и склонениями

**Примеры инструментов:**
- pymorphy2 (морфологический анализ)
- mystem (Yandex анализатор)
- Natasha (интегрированный инструмент)

### 2. Алгоритмы токенизации

#### Regex-based tokenization

Использует регулярные выражения для определения границ токенов.

```python
import re

def regex_tokenize(text):
    pattern = r'\b\w+\b'
    return re.findall(pattern, text)
```

#### Правила на основе словарей

Использует предварительно определенные словари и правила для каждого языка.

#### Статистические методы

Основаны на вероятностных моделях языка (как BPE, SentencePiece).

---

## Нормализация текста

### 1. Основные операции нормализации

#### 1.1 Приведение к нижнему регистру

```python
text = "ПРИВЕТ, Мир!"
normalized = text.lower()
# Результат: "привет, мир!"
```

**Рассмотрения:**
- Теряется информация о собственных именах и аббревиатурах
- Может быть нежелательно в системах, чувствительных к регистру (например, NER)

#### 1.2 Удаление пунктуации и специальных символов

```python
import string

text = "Привет, мир! Как дела?"
text_no_punct = text.translate(str.maketrans('', '', string.punctuation))
# Результат: "Привет мир Как дела"
```

**Рассмотрения:**
- Может быть важна пунктуация для понимания смысла
- Эмотиконы могут содержать информацию в социальных сетях

#### 1.3 Удаление пробелов и стандартизация пробелов

```python
text = "Привет    мир"
normalized = ' '.join(text.split())
# Результат: "Привет мир"
```

#### 1.4 Удаление диакритических знаков (акцентов)

```python
import unicodedata

text = "café"
normalized = ''.join(
    c for c in unicodedata.normalize('NFD', text)
    if unicodedata.category(c) != 'Mn'
)
# Результат: "cafe"
```

**Для русского языка:** менее применимо, но может использоваться для унификации ё/е.

#### 1.5 Стемизация

**Стемизация** — это процесс приведения слова к его основе (стему), путем удаления окончаний и суффиксов.

**Пример:**
```
Слова: "красивый", "красива", "красиво", "красить"
Предполагаемый стем: "крас"
```

**Алгоритмы:**
- Porter Stemmer (английский)
- Snowball (многоязычный)
- Стеммер Портера для русского

**Преимущества:**
- Сокращение размера словаря
- Объединение морфологически связанных слов

**Недостатки:**
- Может быть слишком агрессивным
- "Университет" и "универсам" получат один стем
- Может исказить смысл

#### 1.6 Лемматизация

**Лемматизация** — это приведение слова к его лемме (словарной форме) с использованием морфологического анализа.

**Пример:**
```
Слова: "красивый", "красива", "красиво"
Лемма: "красивый"
```

**Инструменты для русского:**
- pymorphy2
- mystem
- Natasha

**Преимущества:**
- Более точное объединение слов
- Сохранение смысла

**Недостатки:**
- Требует морфологического анализатора
- Медленнее стемизации
- Зависит от качества анализатора

#### 1.7 Расширение сокращений

```python
abbreviations = {
    "Mr.": "Mister",
    "Dr.": "Doctor",
    "U.S.A": "United States of America"
}

def expand_abbreviations(text):
    for abbrev, full in abbreviations.items():
        text = text.replace(abbrev, full)
    return text
```

#### 1.8 Удаление стопсловPython

**Стопслова** — это часто встречающиеся слова (предлоги, союзы, артикли), которые обычно имеют малую информационную ценность.

```python
from nltk.corpus import stopwords

russian_stopwords = set(stopwords.words('russian'))
text = "Это очень важное сообщение для вас"
tokens = text.split()
filtered = [t for t in tokens if t.lower() not in russian_stopwords]
# Результат: ["важное", "сообщение"]
```

**Рассмотрения:**
- Может быть полезно для классификации документов
- Вредно для задач машинного перевода или вопросно-ответных систем
- Зависит от конкретной задачи

---

## Проблемы и вызовы

### 1. Проблемы токенизации

#### 1.1 Нечеткие границы токенов

**Проблема:** Неясно, где заканчивается один токен и начинается другой.

**Примеры:**
- "don't" → ["don", "'t"] или ["don't"] или ["do", "n't"]?
- "U.S.A" → как разбить с учетом точек?
- "mother-in-law" → одно слово или три?

**Решение:** Использование контекстно-зависимых правил и статистических методов.

#### 1.2 Многословные выражения

**Проблема:** Идиоматические выражения и фразовые глаголы не должны разбиваться.

**Примеры:**
- "New York" (город) → должно быть одним токеном
- "take off" (фразовый глагол) → должно рассматриваться как одна единица
- "в конце концов" (идиоматическое выражение)

**Решение:** Использование словарей многословных выражений и техник фразовой токенизации.

#### 1.3 Слова с дефисами и апострофами

**Проблема:** Неправильная обработка символов-разделителей.

**Примеры (русский язык):**
- "какой-то" → разбить или сохранить?
- "что-нибудь" → один или два токена?
- "в чём-то" → как обработать ё с дефисом?

#### 1.4 Числа и специальные последовательности

**Проблема:** Количественная обработка чисел, дат, URL, адресов электронной почты.

**Примеры:**
- "2023-01-15" → одна дата или три числа?
- "user@example.com" → один токен или несколько?
- "1,234.56" → число с разделителями

#### 1.5 Неизвестные слова (Out-of-Vocabulary, OOV)

**Проблема:** Слова, которые не встречаются в训练ном наборе.

**Решения:**
- Использование подсловной токенизации (BPE, WordPiece)
- Замена на специальный токен [UNK]
- Fallback к символам

#### 1.6 Языково-специфичные проблемы

**Для русского языка:**
- Сложная система падежей и склонений
- Приставки и суффиксы изменяют значение
- Ударение может не быть явно обозначено
- Синтетический язык с богатой морфологией

**Для других языков:**
- Китайский и японский: отсутствие пробелов между словами
- Арабский: письмо справа налево
- Корейский: особенности слогового письма

### 2. Проблемы нормализации

#### 2.1 Потеря информации

**Проблема:** Чрезмерная нормализация может привести к потере важной информации.

**Примеры:**
- Нижний регистр: "Apple" (компания) vs "apple" (фрукт)
- Удаление пунктуации: смысл может измениться ("Let's go" vs "Let s go")
- Стемизация: "bank" (банк) и "banking" → один стем, но "bank" (берег) → другой контекст

#### 2.2 Языково-специфичные проблемы нормализации

**Русский язык:**
- Диакритика не обязательна (ё ↔ е)
- Йотированные буквы: я, ю, е, ё
- Мягкий и твердый знаки ь, ъ

**Другие языки:**
- Выбор между нормальной и композитной формой Unicode
- Диакритические знаки могут быть частью письма

#### 2.3 Проблема омонимии при лемматизации

**Проблема:** Одна словоформа может соответствовать нескольким леммам.

**Пример (русский):**
- "стекло" → лемма "стекло" (вещество) или "стечь" (глагол)?
- "замок" → лемма "замок" (архитектура) или "замок" (механизм)?

**Решение:** Использование контекстного разрешения неоднозначности (WSD).

#### 2.4 Проблема производительности

**Проблема:** Некоторые методы нормализации медленные.

**Примеры:**
- Морфологический анализ каждого слова требует времени
- Лемматизация медленнее стемизации
- Может быть узким местом в реальных приложениях

#### 2.5 Выбор уровня нормализации

**Проблема:** Разные задачи требуют разных уровней нормализации.

**Примеры:**
- **Классификация текста:** более агрессивная нормализация
- **Named Entity Recognition:** более консервативная нормализация
- **Machine Translation:** минимальная нормализация
- **Information Retrieval:** промежуточный уровень

---

## Практические упражнения

### Упражнение 1: Простая токенизация

**Задача:** Реализуйте простую словную токенизацию для русского текста.

```python
def simple_tokenize(text):
    """
    Разбивает текст на токены по пробелам и пунктуации.
    
    Параметры:
        text (str): Входной текст
        
    Возвращает:
        list: Список токенов
    """
    # TODO: Реализуйте эту функцию
    pass

# Тестовые примеры
text1 = "Привет, мир! Как дела?"
print(simple_tokenize(text1))
# Ожидаемый результат: ['Привет', ',', 'мир', '!', 'Как', 'дела', '?']

text2 = "U.S.A. — великая страна."
print(simple_tokenize(text2))
# Ожидаемый результат: ['U.S.A', '.', '—', 'великая', 'страна', '.']
```

**Ожидаемые результаты:** Функция должна корректно обрабатывать пунктуацию, пробелы и возвращать список токенов.

---

### Упражнение 2: Нормализация текста

**Задача:** Реализуйте функцию базовой нормализации текста.

```python
def normalize_text(text, lowercase=True, remove_punct=True, 
                   remove_extra_spaces=True):
    """
    Нормализует текст согласно указанным параметрам.
    
    Параметры:
        text (str): Входной текст
        lowercase (bool): Приводить ли в нижний регистр
        remove_punct (bool): Удалять ли пунктуацию
        remove_extra_spaces (bool): Удалять ли лишние пробелы
        
    Возвращает:
        str: Нормализованный текст
    """
    # TODO: Реализуйте эту функцию
    pass

# Тестовые примеры
text = "ПРИВЕТ,  мир!   Как   ДЕЛА???"
result = normalize_text(text)
print(result)
# Ожидаемый результат: "привет мир как дела"
```

---

### Упражнение 3: Стемизация

**Задача:** Реализуйте простой алгоритм стемизации для русского языка.

```python
def simple_stem(word):
    """
    Простой стемизатор на основе удаления суффиксов.
    
    Параметры:
        word (str): Входное слово
        
    Возвращает:
        str: Стем слова
    """
    # Список типичных русских суффиксов
    suffixes = ['ский', 'ой', 'ая', 'ее', 'ая', 'ь', 'ю', 'ой', 
                'ви', 'ус', 'ус', 'ий', 'ое', 'ые', 'ые']
    
    # TODO: Реализуйте удаление суффиксов
    pass

# Тестовые примеры
words = ['красивый', 'красива', 'красиво', 'красить']
for word in words:
    print(f"{word} → {simple_stem(word)}")
```

---

### Упражнение 4: BPE токенизация

**Задача:** Реализуйте упрощенную версию Byte Pair Encoding.

```python
def bpe_tokenize(text, num_merges=10):
    """
    Простая реализация BPE токенизации.
    
    Параметры:
        text (str): Входной текст
        num_merges (int): Количество операций объединения
        
    Возвращает:
        list: Список токенов после BPE
    """
    # TODO: Реализуйте BPE алгоритм
    # 1. Начните с набора всех символов
    # 2. Подсчитайте частоту всех пар
    # 3. Объедините наиболее частую пару
    # 4. Повторите шаги 2-3 num_merges раз
    pass

# Тестовый пример
text = "low low lowest low new new"
tokens = bpe_tokenize(text, num_merges=5)
print(tokens)
```

---

### Упражнение 5: Обработка стопслов

**Задача:** Реализуйте фильтрацию стопслов.

```python
def remove_stopwords(tokens, stopwords=None):
    """
    Удаляет стопслова из списка токенов.
    
    Параметры:
        tokens (list): Список токенов
        stopwords (set): Набор стопслов (если None, используется предопределенный)
        
    Возвращает:
        list: Отфильтрованный список токенов
    """
    if stopwords is None:
        stopwords = {'в', 'на', 'и', 'к', 'из', 'с', 'по', 'из', 'для', 'от'}
    
    # TODO: Реализуйте фильтрацию
    pass

# Тестовый пример
tokens = ['это', 'очень', 'важное', 'сообщение', 'для', 'вас']
filtered = remove_stopwords(tokens)
print(filtered)
# Ожидаемый результат: ['очень', 'важное', 'сообщение', 'вас']
```

---

### Упражнение 6: Обработка OOV токенов

**Задача:** Реализуйте обработку неизвестных слов.

```python
def handle_oov_tokens(tokens, vocabulary, unk_token='[UNK]'):
    """
    Заменяет неизвестные токены на специальный токен.
    
    Параметры:
        tokens (list): Список токенов
        vocabulary (set): Набор известных слов
        unk_token (str): Токен для неизвестных слов
        
    Возвращает:
        list: Список токенов с замененными OOV
    """
    # TODO: Реализуйте замену OOV токенов
    pass

# Тестовый пример
tokens = ['кошка', 'собака', 'xyzabc', 'дерево', 'qwerty']
vocab = {'кошка', 'собака', 'дерево', 'птица'}
result = handle_oov_tokens(tokens, vocab)
print(result)
# Ожидаемый результат: ['кошка', 'собака', '[UNK]', 'дерево', '[UNK]']
```

---

### Упражнение 7: Анализ эффективности нормализации

**Задача:** Оцените влияние нормализации на размер словаря.

```python
def analyze_normalization_impact(texts):
    """
    Анализирует влияние различных методов нормализации на размер словаря.
    
    Параметры:
        texts (list): Список текстов
        
    Возвращает:
        dict: Словарь с размерами для разных уровней нормализации
    """
    results = {}
    
    # TODO: Реализуйте анализ
    # 1. Подсчитайте словарь без нормализации
    # 2. Подсчитайте словарь с нижним регистром
    # 3. Подсчитайте словарь с нижним регистром и удалением пунктуации
    # 4. Подсчитайте словарь с полной нормализацией
    
    return results

# Тестовый пример
texts = [
    "Привет, мир!",
    "привет, Мир!",
    "ПРИВЕТ, МИР!",
    "Привет мир"
]
impact = analyze_normalization_impact(texts)
for norm_level, vocab_size in impact.items():
    print(f"{norm_level}: {vocab_size} уникальных слов")
```

---

### Упражнение 8: Комплексная система токенизации и нормализации

**Задача:** Создайте класс для полного конвейера предварительной обработки текста.

```python
class TextPreprocessor:
    """
    Класс для комплексной предварительной обработки текста.
    """
    
    def __init__(self, remove_stopwords=False, lemmatize=False):
        """
        Инициализация препроцессора.
        
        Параметры:
            remove_stopwords (bool): Удалять ли стопслова
            lemmatize (bool): Проводить ли лемматизацию
        """
        self.remove_stopwords = remove_stopwords
        self.lemmatize = lemmatize
        # TODO: Инициализируйте необходимые инструменты
    
    def preprocess(self, text):
        """
        Полная предварительная обработка текста.
        
        Параметры:
            text (str): Входной текст
            
        Возвращает:
            list: Список обработанных токенов
        """
        # TODO: Реализуйте комплексный процесс:
        # 1. Нормализация текста
        # 2. Токенизация
        # 3. Опциональное удаление стопслов
        # 4. Опциональная лемматизация
        pass

# Тестовый пример
processor = TextPreprocessor(remove_stopwords=True)
text = "Это очень важное сообщение для вас!"
processed = processor.preprocess(text)
print(processed)
```

---

## Контрольные вопросы

### Теоретические вопросы

1. **Основные концепции**
   - Какова разница между токеном и словом?
   - Почему нормализация важна в NLP?
   - Назовите основные уровни токенизации.

2. **Методы токенизации**
   - Объясните принцип работы алгоритма BPE.
   - В чем отличие WordPiece от BPE?
   - Каковы преимущества подсловной токенизации?
   - Почему простая словная токенизация неэффективна для китайского языка?

3. **Методы нормализации**
   - Какова разница между стемизацией и лемматизацией?
   - Приведите примеры, когда лемматизация лучше стемизации.
   - Какие проблемы возникают при удалении стопслов?
   - Как нормализация влияет на размер словаря?

4. **Проблемы и решения**
   - Что такое OOV (Out-of-Vocabulary) проблема и как ее решить?
   - Какие проблемы возникают при обработке многословных выражений?
   - Как обрабатываются аббревиатуры?
   - Почему нельзя применять одинаковую нормализацию ко всем задачам NLP?

5. **Практическое применение**
   - Как бы вы организовали токенизацию для социальных сетей (с эмодзи)?
   - Какие методы нормализации подходят для Named Entity Recognition?
   - Как различаются требования к предварительной обработке в Machine Translation и Text Classification?

### Вопросы на понимание проблем русского языка

6. **Русский язык специфика**
   - Какие сложности в обработке русского текста связаны с его синтетичностью?
   - Как правильно обрабатывать слова с дефисом ("какой-то", "что-нибудь")?
   - Приведите примеры омонимии в русском языке и объясните, как это влияет на лемматизацию.
   - Каковы особенности обработки йотированных букв (я, ю, е, ё)?

### Вопросы для дискуссии

7. **Критическое мышление**
   - Спроектируйте систему токенизации и нормализации для поисковой системы. Какие решения вы бы приняли?
   - Как вы бы обработали текст из Интернета с различными кодировками и ошибками?
   - Какую роль играет контекст при выборе способа нормализации?
   - Как вы оценили бы качество токенизации и нормализации?

### Задачи с кодом

8. **Написание кода**
   - Реализуйте токенизатор, который корректно обрабатывает URL и адреса электронной почты.
   - Напишите функцию для обнаружения и расширения сокращений на основе словаря.
   - Создайте систему для обработки многословных выражений.
   - Реализуйте простой морфологический анализатор для русского языка.

---

## Литература и ссылки

1. **Основные статьи:**
   - Sennrich, R., Haddow, B., & Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units. ACL 2016.
   - Kudo, T., & Richardson, J. (2018). SentencePiece: A simple and language agnostic approach to subword segmentation. ACL 2018.

2. **Книги:**
   - Jurafsky, D., & Martin, J. H. (2021). Speech and Language Processing (3rd ed.).
   - Bird, S., Klein, E., & Loper, E. (2009). Natural Language Processing with Python.

3. **Инструменты:**
   - NLTK: https://www.nltk.org/
   - spaCy: https://spacy.io/
   - Hugging Face Tokenizers: https://huggingface.co/docs/tokenizers/
   - pymorphy2: https://github.com/kmike/pymorphy2
   - Natasha: https://natasha.github.io/

4. **Статьи на русском:**
   - Обработка естественного языка в Python (Habr)
   - Лемматизация и стемизация в Python (Medium)

---

## Заключение

Токенизация и нормализация текста являются фундаментальными компонентами любого NLP конвейера. Их правильная реализация критична для успеха всей системы. Выбор методов должен быть обоснован конкретной задачей и особенностями обрабатываемого языка.

Ключевые выводы:
1. Разные задачи требуют разных подходов к нормализации
2. Подсловная токенизация более универсальна, чем словная
3. Лемматизация предпочтительнее стемизации при наличии морфологического анализатора
4. Русский язык имеет специфические особенности, требующие особых подходов
5. Оценка качества предварительной обработки критична для оптимизации всей системы