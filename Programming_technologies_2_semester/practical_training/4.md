# –ü—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

**–ü—Ä–µ–¥–º–µ—Ç:** –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è  
**–°–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å:** –ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞  
–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–∫  



## –í–≤–µ–¥–µ–Ω–∏–µ

–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞ (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è) –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è) ‚Äî —ç—Ç–æ –æ–¥–Ω–∞ –∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–¥–∞—á –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP). –≠—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å—ã —è–≤–ª—è—é—Ç—Å—è –ø–µ—Ä–≤—ã–º —ç—Ç–∞–ø–æ–º –≤–æ –º–Ω–æ–≥–∏—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞, –≤–∫–ª—é—á–∞—è –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏, –º–∞—à–∏–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

### –¶–µ–ª–∏ —É—Ä–æ–∫–∞

- –ü–æ–Ω–∏–º–∞—Ç—å —Ä–∞–∑–ª–∏—á–∏–µ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π —Å–ª–æ–≤ –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –±–∞–∑–æ–≤—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
- –†–∞–∑–æ–±—Ä–∞—Ç—å—Å—è –≤ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤
- –†–µ—à–∞—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∏



## –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã

### 1.1 –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å–ª–æ–≤–∞)

**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è ‚Äî —ç—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (—Ç–æ–∫–µ–Ω—ã), –æ–±—ã—á–Ω–æ —Å–ª–æ–≤–∞, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏–ª–∏ —Å—É–±—Å–ª–æ–≤–∞.

#### –¢–∏–ø—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:

**–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–æ–±–µ–ª–∞–º:**
```
–¢–µ–∫—Å—Ç: "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?"
–†–µ–∑—É–ª—å—Ç–∞—Ç: ["–ü—Ä–∏–≤–µ—Ç,", "–∫–∞–∫", "–¥–µ–ª–∞?"]
```

**–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –æ—Ç–¥–µ–ª–µ–Ω–∏–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏:**
```
–¢–µ–∫—Å—Ç: "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?"
–†–µ–∑—É–ª—å—Ç–∞—Ç: ["–ü—Ä–∏–≤–µ—Ç", ",", "–∫–∞–∫", "–¥–µ–ª–∞", "?"]
```

**–°—É–±—Å–ª–æ–≤–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (BPE, WordPiece):**
```
–¢–µ–∫—Å—Ç: "—Ä–∞–∑–≥–æ–≤–æ—Ä—á–∏–≤—ã–π"
–†–µ–∑—É–ª—å—Ç–∞—Ç: ["—Ä–∞–∑", "–≥–æ–≤–æ—Ä", "—á–∏", "–≤—ã–π"]
```

### 1.2 –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π ‚Äî —ç—Ç–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –≥—Ä–∞–Ω–∏—á–Ω—ã–º –º–∞—Ä–∫–µ—Ä–∞–º (—Ç–æ—á–∫–∞, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫, –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫).

#### –°–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏:

1. **–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã:** "–°—Ä. —Å—Ç–∞—Ç—å—é –Ω–∏–∂–µ" ‚Äî —Ç–æ—á–∫–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –≥—Ä–∞–Ω–∏—Ü–µ–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
2. **–ß–∏—Å–ª–∞:** "1.5 –∫–º" ‚Äî —Ç–æ—á–∫–∞ –≤ —á–∏—Å–ª–µ
3. **–í–µ–±-–∞–¥—Ä–µ—Å–∞ –∏ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –ø–æ—á—Ç–∞:** "info@example.com"
4. **–ú–Ω–æ–≥–æ—Ç–æ—á–∏–µ:** "–ü–æ–≥–æ–¥–∏—Ç–µ... –ß—Ç–æ —ç—Ç–æ?"
5. **–ö–∞–≤—ã—á–∫–∏ –∏ —Å–∫–æ–±–∫–∏:** –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –º–æ–∂–µ—Ç –∑–∞–∫–∞–Ω—á–∏–≤–∞—Ç—å—Å—è –≤–Ω—É—Ç—Ä–∏ –Ω–∏—Ö

### 1.3 –õ–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã

**–†—É—Å—Å–∫–∏–π —è–∑—ã–∫:**
- –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã: "—Ç—ã—Å.", "—Ç.–µ.", "–≥-–Ω"
- –°–æ–∫—Ä–∞—â–µ–Ω–∏—è: "–ü–µ—Ç—Ä I" (–ª–∞—Ç–∏–Ω—Å–∫–∞—è —Ü–∏—Ñ—Ä–∞)
- –ö–ª–∏—Ç–∏–∫–∏: "-–ª–∏", "-–∂–µ"

**–ê–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫:**
- –ü—Ä–∏—Ç—è–∂–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã: "John's", "students'"
- –°–æ–∫—Ä–∞—â–µ–Ω–∏—è: "don't", "it's"
- –ú–Ω–æ–≥–æ—Å–ª–æ–∂–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è: "New York"



## –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–∫

### –ü—Ä–∏–º–µ—Ä 1: –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –Ω–∞ Python

```python
def simple_tokenization(text):
    """
    –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–æ–±–µ–ª–∞–º
    
    Args:
        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
    """
    tokens = text.split()
    return tokens


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
text = "–û–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ - —ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ!"
result = simple_tokenization(text)
print(result)
# –í—ã–≤–æ–¥: ['–û–±—Ä–∞–±–æ—Ç–∫–∞', '–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ', '—è–∑—ã–∫–∞', '-', '—ç—Ç–æ', '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ!']
```

### –ü—Ä–∏–º–µ—Ä 2: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –æ—Ç–¥–µ–ª–µ–Ω–∏–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏

```python
import re

def advanced_tokenization(text):
    """
    –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –æ—Ç–¥–µ–ª–µ–Ω–∏–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    
    Args:
        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
    """
    # –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ: –æ—Ç–¥–µ–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    pattern = r'(\w+|[.,!?;:\-‚Äî])'
    tokens = re.findall(pattern, text)
    return tokens


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
text = "–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞ –∏–∑—É—á–∞–µ—Ç —è–∑—ã–∫. –≠—Ç–æ —Å–ª–æ–∂–Ω–æ, –Ω–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ!"
result = advanced_tokenization(text)
print(result)
# –í—ã–≤–æ–¥: ['–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è', '–ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞', '–∏–∑—É—á–∞–µ—Ç', '—è–∑—ã–∫', '.', 
#         '–≠—Ç–æ', '—Å–ª–æ–∂–Ω–æ', ',', '–Ω–æ', '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '!']
```

### –ü—Ä–∏–º–µ—Ä 3: –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

```python
import re

def simple_sentence_segmentation(text):
    """
    –ü—Ä–æ—Å—Ç–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    
    Args:
        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    """
    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ç–æ—á–∫–µ, –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω–æ–º—É –∏ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–º—É –∑–Ω–∞–∫–∞–º
    sentences = re.split(r'[.!?]+', text)
    # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    sentences = [s.strip() for s in sentences if s.strip()]
    return sentences


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
text = "–ü–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ. –í—Ç–æ—Ä–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ? –¢—Ä–µ—Ç—å–µ!"
result = simple_sentence_segmentation(text)
for i, sent in enumerate(result, 1):
    print(f"{i}. {sent}")
# –í—ã–≤–æ–¥:
# 1. –ü–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
# 2. –í—Ç–æ—Ä–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
# 3. –¢—Ä–µ—Ç—å–µ
```

### –ü—Ä–∏–º–µ—Ä 4: –£–º–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä

```python
import re

class SmartSentenceSegmenter:
    """
    –£–º–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
    """
    
    def __init__(self):
        # –°–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
        self.abbreviations = {
            '—Ç—ã—Å.', '–º–ª–Ω.', '–º–ª—Ä–¥.', '—Ç.–µ.', '—Ç.–¥.', '—Ç.–ø.',
            '–¥—Ä.', '–∏ —Ç.–¥.', '–∏ —Ç.–ø.', '—Å—Ä.', '—Å–º.', '—Ä–∏—Å.',
            '–ø—Ä–∏–º–µ—á.', '–∞–≤—Ç.', '–≤—Ç.', '—á.', '–≥.', '–≥—Ä.',
            '–ø—Ä–æ—Ñ.', '–≥-–Ω', '–≥-–∂–∞', '–≥-–∂.', '—Å—Ç—Ä.', '–≥–ª.'
        }
    
    def segment(self, text):
        """
        –†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        
        Args:
            text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            list: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        """
        # –ó–∞—â–∏—â–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏
        protected_text = text
        markers = {}
        
        for i, abbr in enumerate(self.abbreviations):
            marker = f"__ABBR_{i}__"
            markers[marker] = abbr
            protected_text = protected_text.replace(abbr, marker)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        sentences = re.split(r'([.!?])\s+', protected_text)
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        result_sentences = []
        current_sentence = ""
        
        for i, part in enumerate(sentences):
            current_sentence += part
            
            # –ï—Å–ª–∏ —ç—Ç–æ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è, –∑–∞–≤–µ—Ä—à–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            if part in '.!?':
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∞—Ä–∫–µ—Ä—ã
                for marker, abbr in markers.items():
                    current_sentence = current_sentence.replace(marker, abbr)
                
                if current_sentence.strip():
                    result_sentences.append(current_sentence.strip())
                current_sentence = ""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å
        if current_sentence.strip():
            for marker, abbr in markers.items():
                current_sentence = current_sentence.replace(marker, abbr)
            result_sentences.append(current_sentence.strip())
        
        return result_sentences


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
segmenter = SmartSentenceSegmenter()
text = "–ö–æ–º–ø–∞–Ω–∏—è –ø–æ–ª—É—á–∏–ª–∞ 1.5 –º–ª–Ω. —Ä—É–±–ª–µ–π. –≠—Ç–æ –±–æ–ª—å—à–∞—è —Å—É–º–º–∞. –ß—Ç–æ –¥–∞–ª—å—à–µ?"
sentences = segmenter.segment(text)
for i, sent in enumerate(sentences, 1):
    print(f"{i}. {sent}")
```

### –ü—Ä–∏–º–µ—Ä 5: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ NLTK –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞

```python
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞: pip install nltk

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords

# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
nltk.download('punkt')
nltk.download('stopwords')

def tokenize_with_nltk(text, language='russian'):
    """
    –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é NLTK
    
    Args:
        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        language (str): –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞
        
    Returns:
        tuple: (—Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤)
    """
    # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    sentences = sent_tokenize(text, language=language)
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
    tokens = word_tokenize(text, language=language)
    
    return sentences, tokens


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
text = """–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞ –∏–∑—É—á–∞–µ—Ç –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤. 
–û–Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ –º–µ—Ç–æ–¥—ã. –≠—Ç–æ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å!"""

sentences, tokens = tokenize_with_nltk(text)

print("–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:")
for i, sent in enumerate(sentences, 1):
    print(f"{i}. {sent}")

print("\n–¢–æ–∫–µ–Ω—ã:")
print(tokens)
```

### –ü—Ä–∏–º–µ—Ä 6: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ spaCy

```python
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞: pip install spacy
# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: python -m spacy download ru_core_news_sm

import spacy

def process_with_spacy(text):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é spaCy
    
    Args:
        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
    Returns:
        spacy Doc: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    nlp = spacy.load("ru_core_news_sm")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    doc = nlp(text)
    
    return doc


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
text = "–ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ü—É—à–∫–∏–Ω –Ω–∞–ø–∏—Å–∞–ª \"–ï–≤–≥–µ–Ω–∏—è –û–Ω–µ–≥–∏–Ω–∞\" –≤ 1833 –≥–æ–¥—É. –≠—Ç–æ –≤–µ–ª–∏–∫–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ!"

doc = process_with_spacy(text)

print("–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:")
for i, sent in enumerate(doc.sents, 1):
    print(f"{i}. {sent.text}")

print("\n–¢–æ–∫–µ–Ω—ã (—Å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π):")
for token in doc:
    print(f"{token.text:15} -> {token.lemma_:15} | POS: {token.pos_}")
```

### –ü—Ä–∏–º–µ—Ä 7: –ü–æ–ª–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞

```python
import re
from collections import Counter

class TextProcessor:
    """
    –ü–æ–ª–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
    """
    
    def __init__(self):
        self.text = ""
        self.sentences = []
        self.tokens = []
        self.vocabulary = Counter()
    
    def load_text(self, text):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç"""
        self.text = text
        return self
    
    def segment_sentences(self):
        """–†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        pattern = r'[.!?]+'
        sentences = re.split(pattern, self.text)
        self.sentences = [s.strip() for s in sentences if s.strip()]
        return self
    
    def tokenize(self):
        """–¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ"""
        for sentence in self.sentences:
            # –û—Ç–¥–µ–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
            tokens = re.findall(r'\w+|[.,!?;:\-‚Äî]', sentence, re.UNICODE)
            self.tokens.extend(tokens)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤
        self.vocabulary = Counter([t.lower() for t in self.tokens if t.isalnum()])
        return self
    
    def get_sentences(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        return self.sentences
    
    def get_tokens(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤"""
        return self.tokens
    
    def get_vocabulary(self, top_n=10):
        """–ü–æ–ª—É—á–∏—Ç—å —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞"""
        return self.vocabulary.most_common(top_n)
    
    def get_statistics(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return {
            'sentences': len(self.sentences),
            'tokens': len(self.tokens),
            'unique_words': len(self.vocabulary),
            'avg_sentence_length': len(self.tokens) / len(self.sentences) if self.sentences else 0
        }


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    text = """–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞ ‚Äî —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –Ω–∞—É–∫–∏, –Ω–∞—Ö–æ–¥—è—â–∞—è—Å—è –Ω–∞ —Å—Ç—ã–∫–µ 
    –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∏ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏. –û–Ω–∞ –∏–∑—É—á–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤. 
    –ì–ª–∞–≤–Ω–∞—è —Ü–µ–ª—å ‚Äî –Ω–∞—É—á–∏—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä –ø–æ–Ω–∏–º–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫—É—é —Ä–µ—á—å."""
    
    processor = TextProcessor()
    processor.load_text(text).segment_sentences().tokenize()
    
    print("=== –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø ===")
    for i, sent in enumerate(processor.get_sentences(), 1):
        print(f"{i}. {sent}")
    
    print("\n=== –¢–û–ö–ï–ù–´ ===")
    print(processor.get_tokens())
    
    print("\n=== –¢–û–ü-10 –°–õ–û–í ===")
    for word, freq in processor.get_vocabulary(10):
        print(f"{word}: {freq}")
    
    print("\n=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===")
    stats = processor.get_statistics()
    for key, value in stats.items():
        print(f"{key}: {value:.2f}" if isinstance(value, float) else f"{key}: {value}")
```



## –ß–∞—Å—Ç—å 3: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è

### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 1: –ë–∞–∑–æ–≤–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è

**–ó–∞–¥–∞—á–∞:** –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞, —É—á–∏—Ç—ã–≤–∞—è –ø—Ä–æ–±–µ–ª—ã –∏ —Ç–∞–±—É–ª—è—Ü–∏–∏.

```python
def exercise_1(text):
    """
    TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –ø–æ –ø—Ä–æ–±–µ–ª–∞–º
    
    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - –†–∞–∑–¥–µ–ª–∏—Ç—å —Ç–µ–∫—Å—Ç –ø–æ –ø—Ä–æ–±–µ–ª–∞–º, —Ç–∞–±—É–ª—è—Ü–∏—è–º –∏ –ø–µ—Ä–µ–≤–æ–¥–∞–º —Å—Ç—Ä–æ–∫
    - –£–¥–∞–ª–∏—Ç—å –ø—É—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã
    - –í–µ—Ä–Ω—É—Ç—å —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
    """
    pass


# –¢–µ—Å—Ç
test_text = "–ü—Ä–∏–≤–µ—Ç   –º–∏—Ä!\t–≠—Ç–æ\n—Ç–µ—Å—Ç"
expected = ["–ü—Ä–∏–≤–µ—Ç", "–º–∏—Ä!", "–≠—Ç–æ", "—Ç–µ—Å—Ç"]
# assert exercise_1(test_text) == expected
```

**–ü–æ–¥—Å–∫–∞–∑–∫–∞:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç–æ–¥—ã —Å—Ç—Ä–æ–∫ `.split()` –∏–ª–∏ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è.



### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 2: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏

**–ó–∞–¥–∞—á–∞:** –°–æ–∑–¥–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –æ—Ç–¥–µ–ª—è–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –æ—Ç —Å–ª–æ–≤.

```python
def exercise_2(text):
    """
    TODO: –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —Å –æ—Ç–¥–µ–ª–µ–Ω–∏–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    
    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - –û—Ç–¥–µ–ª–∏—Ç—å –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ (. , ! ? ; :) –æ—Ç —Å–ª–æ–≤
    - –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–µ—Ñ–∏—Å—ã –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–∫–∞–∫–æ–π-—Ç–æ")
    - –í–µ—Ä–Ω—É—Ç—å —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
    
    –ü—Ä–∏–º–µ—Ä:
    "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!" -> ["–ü—Ä–∏–≤–µ—Ç", ",", "–º–∏—Ä", "!"]
    """
    pass


# –¢–µ—Å—Ç
test_cases = [
    ("–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!", ["–ü—Ä–∏–≤–µ—Ç", ",", "–º–∏—Ä", "!"]),
    ("–ö–∞–∫–æ–π-—Ç–æ —Ç–µ–∫—Å—Ç.", ["–ö–∞–∫–æ–π-—Ç–æ", "—Ç–µ–∫—Å—Ç", "."]),
    ("–≠—Ç–æ –≤–æ–ø—Ä–æ—Å?", ["–≠—Ç–æ", "–≤–æ–ø—Ä–æ—Å", "?"])
]
```

**–ü–æ–¥—Å–∫–∞–∑–∫–∞:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è —Å –º–µ—Ç–æ–¥–æ–º `re.findall()`.



### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 3: –ü—Ä–æ—Å—Ç–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

**–ó–∞–¥–∞—á–∞:** –†–∞–∑–¥–µ–ª–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º.

```python
def exercise_3(text):
    """
    TODO: –†–∞–∑–¥–µ–ª–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    
    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - –†–∞–∑–¥–µ–ª–∏—Ç—å –ø–æ —Ç–æ—á–∫–∞–º, –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∏ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–∞–º
    - –£–¥–∞–ª–∏—Ç—å –ø—É—Å—Ç—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    - –£–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
    - –í–µ—Ä–Ω—É—Ç—å —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    
    –ü—Ä–∏–º–µ—Ä:
    "–ü–µ—Ä–≤–æ–µ. –í—Ç–æ—Ä–æ–µ?" -> ["–ü–µ—Ä–≤–æ–µ", "–í—Ç–æ—Ä–æ–µ"]
    """
    pass


# –¢–µ—Å—Ç
text = "–ö–æ–º–ø—å—é—Ç–µ—Ä - –ø–æ–º–æ—â–Ω–∏–∫ —á–µ–ª–æ–≤–µ–∫–∞. –û–Ω —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á–∏. –ö–∞–∫–∏–µ –∑–∞–¥–∞—á–∏?"
# expected = ["–ö–æ–º–ø—å—é—Ç–µ—Ä - –ø–æ–º–æ—â–Ω–∏–∫ —á–µ–ª–æ–≤–µ–∫–∞", "–û–Ω —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á–∏", "–ö–∞–∫–∏–µ –∑–∞–¥–∞—á–∏"]
```



### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 4: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä

**–ó–∞–¥–∞—á–∞:** –†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –ø—Ä–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏.

```python
def exercise_4(text, abbreviations):
    """
    TODO: –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
    
    Args:
        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        abbreviations (list): –°–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
        
    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - –£—á–∏—Ç—ã–≤–∞—Ç—å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏
    - –ù–µ —Ä–∞–∑–±–∏–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
    - –í–µ—Ä–Ω—É—Ç—å —Å–ø–∏—Å–æ–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    
    –ü—Ä–∏–º–µ—Ä:
    text = "–ü–æ–ª—É—á–µ–Ω–æ 1.5 –º–ª–Ω. —Ä—É–±–ª–µ–π. –≠—Ç–æ —Å—É–º–º–∞!"
    abbreviations = ['–º–ª–Ω.', '–º–ª—Ä–¥.', '—Ç—ã—Å.']
    expected = ["–ü–æ–ª—É—á–µ–Ω–æ 1.5 –º–ª–Ω. —Ä—É–±–ª–µ–π", "–≠—Ç–æ —Å—É–º–º–∞"]
    """
    pass
```



### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 5: –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏

**–ó–∞–¥–∞—á–∞:** –ù–∞–ø–∏—Å–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ç–æ–∫–µ–Ω–∞–º –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º.

```python
def exercise_5(text):
    """
    TODO: –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ–∫—Å—Ç–∞
    
    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - –†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    - –†–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ —Ç–æ–∫–µ–Ω—ã
    - –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å:
      * –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
      * –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ (—Ç–æ–∫–µ–Ω–æ–≤ –±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏)
      * –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
      * –°—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ö
      * –¢–æ–ø-5 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤
    
    –í–µ—Ä–Ω—É—Ç—å —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
    {
        'sentences_count': int,
        'words_count': int,
        'unique_words': int,
        'avg_sentence_length': float,
        'top_5_words': list of tuples (word, frequency)
    }
    """
    pass


# –¢–µ—Å—Ç
text = "–î–∞–Ω–Ω—ã–µ - —ç—Ç–æ –Ω–æ–≤–æ–µ –∑–æ–ª–æ—Ç–æ. –î–∞–Ω–Ω—ã–µ –ø–æ–º–æ–≥–∞—é—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è. –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∂–µ–Ω!"
```



### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 6: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤

**–ó–∞–¥–∞—á–∞:** –£–¥–∞–ª–∏—Ç—å –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (—Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞).

```python
def exercise_6(text, stopwords=None):
    """
    TODO: –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ —É–¥–∞–ª–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    
    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç
    - –£–¥–∞–ª–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–ø—Ä–µ–¥–ª–æ–≥–∏, —Å–æ—é–∑—ã, –∞—Ä—Ç–∏–∫–ª–∏ –∏ —Ç.–¥.)
    - –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    - –í–µ—Ä–Ω—É—Ç—å —Å–ø–∏—Å–æ–∫ –∑–Ω–∞—á–∏–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    
    –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é:
    –∏, –∞, –æ, –≤, –Ω–∞, —Å, –∏–∑, –∫, –ø–æ, —É, –¥–ª—è, –æ—Ç, –∫–∞–∫, —á—Ç–æ, —ç—Ç–æ, 
    —Ç–µ, —Ç–∞, —Ç–æ, —Ç–æ, –Ω–æ, –∂–µ, –ª–∏, —Ç—ã, —è, –æ–Ω, –æ–Ω–∞, –æ–Ω–æ, –æ–Ω–∏, –º—ã, –≤—ã
    """
    if stopwords is None:
        stopwords = {'–∏', '–∞', '–æ', '–≤', '–Ω–∞', '—Å', '–∏–∑', '–∫', '–ø–æ', '—É', 
                     '–¥–ª—è', '–æ—Ç', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '–Ω–æ', '–∂–µ', '–ª–∏'}
    pass
```



### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 7: –ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ (–ü–†–û–î–í–ò–ù–£–¢–û–ï)

**–ó–∞–¥–∞—á–∞:** –°–æ–∑–¥–∞—Ç—å –∫–ª–∞—Å—Å `AdvancedTextProcessor` —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏:

```python
class AdvancedTextProcessor:
    """
    TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ç–µ–∫—Å—Ç–∞
    
    –ú–µ—Ç–æ–¥—ã:
    - __init__(language='russian')
    - load_text(text) -> self (–¥–ª—è —Ü–µ–ø–æ—á–∫–∏ –≤—ã–∑–æ–≤–æ–≤)
    - segment() -> self
    - tokenize() -> self
    - normalize() -> self (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, —É–¥–∞–ª–µ–Ω–∏–µ –¥–∏–∞–∫—Ä–∏—Ç–∏–∫–∏)
    - remove_stopwords(stopwords_list) -> self
    - get_sentences() -> list
    - get_tokens() -> list
    - get_unique_words() -> set
    - get_word_frequencies() -> dict
    - get_statistics() -> dict
    
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    processor = AdvancedTextProcessor('russian')
    result = processor.load_text(text).segment().tokenize().get_statistics()
    """
    pass
```



### –£–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ 8: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ (–ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–°–ö–û–ï)

**–ó–∞–¥–∞—á–∞:** –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ —Å–ª—É—á–∞–∏ –≤ —Ç–µ–∫—Å—Ç–∞—Ö:

```python
def exercise_8_handle_special_cases():
    """
    TODO: –†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:
    
    1. –í–µ–±-–∞–¥—Ä–µ—Å–∞: https://example.com, www.site.ru
    2. –≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –ø–æ—á—Ç–∞: user@example.com
    3. –•–µ—à—Ç–µ–≥–∏: #–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è–ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞
    4. –£–ø–æ–º–∏–Ω–∞–Ω–∏—è: @username
    5. –¶–∏—Ñ—Ä—ã –∏ —á–∏—Å–ª–∞: 1000, 3.14, 1,000.50
    6. –≠–º–æ–¥–∑–∏: üòä üëç
    7. URL –≤ —Å–∫–æ–±–∫–∞—Ö: (https://example.com)
    8. –¶–µ–Ω—ã: $100, ‚Ç¨50, ‚ÇΩ1000
    
    –°–æ–∑–¥–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é tokenize_with_special_handling(text)
    –∫–æ—Ç–æ—Ä–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç—Ç–∏ —Å–ª—É—á–∞–∏.
    """
    pass


# –¢–µ—Å—Ç
test_texts = [
    "–ü–æ—Å–µ—Ç–∏—Ç–µ https://example.com –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.",
    "–°–≤—è–∂–∏—Ç–µ—Å—å —Å –Ω–∞–º–∏: info@company.ru",
    "–ù–æ–≤—ã–π –ø–æ—Å—Ç #–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è–ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞ –±—ã–ª –ø–æ–ø—É–ª—è—Ä–µ–Ω! üëç",
    "–¶–µ–Ω–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç $100 –∏–ª–∏ ‚Ç¨85.",
    "–†–µ–∑—É–ª—å—Ç–∞—Ç: 3.14 * 100 = 314"
]
```




## –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã

### –í–æ–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–Ω–∏–º–∞–Ω–∏—è

1. **–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –≤ —á–µ–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Å–∏–º–≤–æ–ª—å–Ω–æ–π –∏ —Å–ª–æ–≤–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π?**

2. **–ù–∞–∑–æ–≤–∏—Ç–µ —Ç—Ä–∏ —Ç–∏–ø–∞ –∑–∞–¥–∞—á, –≥–¥–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —Å–ª–æ–≤–∞.**

3. **–ö–∞–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –≤–æ–∑–Ω–∏–∫–∞—é—Ç –ø—Ä–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ?**

4. **–ü—Ä–∏–≤–µ–¥–∏—Ç–µ –ø—Ä–∏–º–µ—Ä—ã –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏.**

5. **–ö–∞–∫–∏–µ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π –≤ –Ω–∞—á–∞–ª–µ, –∫–æ–Ω—Ü–µ –∏ –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞?**

6. **–û–±—ä—è—Å–Ω–∏—Ç–µ, –ø–æ—á–µ–º—É –ø—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.**

7. **–ö–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω—É–∂–Ω—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π?**

8. **–ú–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–Ω—É –∏ —Ç—É –∂–µ –º–æ–¥–µ–ª—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤? –ü–æ—á–µ–º—É?**

9. **–ß—Ç–æ —Ç–∞–∫–æ–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∑–∞—á–µ–º –∏—Ö —É–¥–∞–ª—è—é—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞?**

10. **–û–ø–∏—Å–∞—Ç—å —Ä–∞–∑–ª–∏—á–∏–µ –º–µ–∂–¥—É –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π.**



### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã

11. **–ù–∞–ø–∏—à–∏—Ç–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –≤—Å–µ—Ö URL –≤ —Ç–µ–∫—Å—Ç–µ.**

12. **–ö–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å–ª—É—á–∞–π, –∫–æ–≥–¥–∞ –Ω–∞–∑–≤–∞–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –ø—Ä–æ–±–µ–ª ("–ù—å—é-–ô–æ—Ä–∫", "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥")?**

13. **–û–±—ä—è—Å–Ω–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–∞—Ä–∫–µ—Ä–æ–≤ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä (–∫–∞–∫ –≤ –ü—Ä–∏–º–µ—Ä–µ 4).**

14. **–ö–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π?**

15. **–ö–∞–∫ —É—á–∏—Ç—ã–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ "–Ω–µ –∑–Ω–∞—é")?**



### –í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏—è

16. **–ö–∞–∫–∏–µ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —è–≤–ª–µ–Ω–∏—è –¥–µ–ª–∞—é—Ç –∑–∞–¥–∞—á—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω–æ–π?**

17. **–ú–æ–∂–Ω–æ –ª–∏ —Å–æ–∑–¥–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –¥–ª—è –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤? –ü–æ—á–µ–º—É?**

18. **–ö–∞–∫ —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è –ø–æ–¥—Ö–æ–¥—ã –¥–ª—è –ø–∏—Å—å–º–µ–Ω–Ω–æ–π –∏ —É—Å—Ç–Ω–æ–π —Ä–µ—á–∏?**

19. **–ö–∞–∫—É—é —Ä–æ–ª—å –∏–≥—Ä–∞–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –≤ –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∑–∞–¥–∞—á–∞—Ö NLP?**

20. **–û–ø–∏—à–∏—Ç–µ, –∫–∞–∫ –≤—ã –±—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ —Å–∏—Å—Ç–µ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–∞–ª–æ–∏–∑—É—á–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ —Ä–µ—Å—É—Ä—Å–∞–º–∏.**



## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

### –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞

**NLTK (Natural Language Toolkit)**
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://www.nltk.org/
- –£—Å—Ç–∞–Ω–æ–≤–∫–∞: `pip install nltk`
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π —Å —Ç–µ–∫—Å—Ç–æ–º

**spaCy**
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://spacy.io/
- –£—Å—Ç–∞–Ω–æ–≤–∫–∞: `pip install spacy`
- –û—Ç–ª–∏—á–Ω—ã–π –≤—ã–±–æ—Ä –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞

**TextBlob**
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://textblob.readthedocs.io/
- –£—Å—Ç–∞–Ω–æ–≤–∫–∞: `pip install textblob`
- –ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö

**Pymorphy2** (–¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞)
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://pymorphy2.readthedocs.io/
- –£—Å—Ç–∞–Ω–æ–≤–∫–∞: `pip install pymorphy2`
- –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è

### –î–∞—Ç–∞—Å–µ—Ç—ã

- **Russian Corpus** - –ù–ö–†–Ø (–ù–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ—Ä–ø—É—Å —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞)
- **Universal Treebank** - –º–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –∫–æ—Ä–ø—É—Å—ã
- **OpenCorpora** - –æ—Ç–∫—Ä—ã—Ç—ã–π –∫–æ—Ä–ø—É—Å —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞

- Jurafsky, D., & Martin, J. H. (2009). Speech and Language Processing
- –ì–ª–∞–¥–∫–∏–π, –ê. –í. (1985). –°–∏–Ω—Ç–∞–∫—Å–∏—Å –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
- –ö—É–ª–µ–≤, –°. –ê., & –ú—É—Å–∞—Ç–æ–≤–∞, –°. –í. (2017). –ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–∞—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∞



## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–ü—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å–ª–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π ‚Äî —ç—Ç–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –¥–ª—è –≤—Å–µ–π –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞. –£–º–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–π –ª–∏–Ω–≥–≤–∏—Å—Ç–∏–∫–∏.

–ù–∞ —ç—Ç–æ–º —É—Ä–æ–∫–µ –º—ã –∏–∑—É—á–∏–ª–∏:
- –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
- –†–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∏ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
- –ì–æ—Ç–æ–≤—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞
- –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏