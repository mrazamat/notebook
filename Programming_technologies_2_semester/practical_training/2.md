## Тема: Поиск в корпусе текста
### Дисциплина: Технологии программирования
### Специальность: Компьютерная лингвистика

---

## 1. ВВЕДЕНИЕ В КОРПУСНУЮ ЛИНГВИСТИКУ И ПОИСК

**Корпус текстов** — это большая коллекция текстов, собранная по определённым принципам и представленная в электронном виде для лингвистического анализа.

### 1.1 Типы корпусов
- **Общие корпуса** (напр., Национальный корпус русского языка)
- **Специализированные корпуса** (научные тексты, художественная литература)
- **Параллельные корпуса** (для машинного перевода)
- **Диахронические корпуса** (исторические изменения языка)
- **Разговорные корпуса** (устная речь)

### 1.2 Задачи поиска в корпусе
1. Поиск словоформ и лемм
2. Контекстный анализ (конкорданс)
3. Поиск коллокаций
4. Статистический анализ частотности
5. Семантический поиск

---

## 2. ТИПЫ ПОИСКА В КОРПУСЕ

### 2.1 Простой поиск (String Matching)

**Точное совпадение:**
```python
def simple_search(corpus, query):
    results = []
    for doc_id, text in enumerate(corpus):
        if query in text:
            results.append((doc_id, text))
    return results
```

**Регистронезависимый поиск:**
```python
def case_insensitive_search(corpus, query):
    query_lower = query.lower()
    results = []
    for doc_id, text in enumerate(corpus):
        if query_lower in text.lower():
            results.append((doc_id, text))
    return results
```

### 2.2 Поиск с использованием регулярных выражений

```python
import re

def regex_search(corpus, pattern):
    """
    Поиск по регулярному выражению
    """
    compiled_pattern = re.compile(pattern, re.IGNORECASE)
    results = []
    
    for doc_id, text in enumerate(corpus):
        matches = compiled_pattern.finditer(text)
        for match in matches:
            context_start = max(0, match.start() - 50)
            context_end = min(len(text), match.end() + 50)
            context = text[context_start:context_end]
            results.append({
                'doc_id': doc_id,
                'match': match.group(),
                'position': match.start(),
                'context': context
            })
    return results

# Примеры использования:
# Поиск слов, начинающихся с "пред-"
pattern1 = r'\bпред\w+'

# Поиск всех форм слова "идти"
pattern2 = r'\b(ид|шёл|шла|шли|пойд)\w*'
```

### 2.3 Лингвистический поиск

#### 2.3.1 Лемматизация и морфологический анализ

```python
import pymorphy2

class MorphologicalSearcher:
    def __init__(self):
        self.morph = pymorphy2.MorphAnalyzer()
    
    def lemmatize(self, word):
        """Получение нормальной формы слова"""
        parsed = self.morph.parse(word)[0]
        return parsed.normal_form
    
    def search_by_lemma(self, corpus, lemma):
        """Поиск всех словоформ по лемме"""
        results = []
        
        for doc_id, text in enumerate(corpus):
            words = text.split()
            for i, word in enumerate(words):
                word_clean = re.sub(r'[^\w]', '', word)
                if self.lemmatize(word_clean) == lemma:
                    # Контекст: 5 слов до и после
                    context_start = max(0, i - 5)
                    context_end = min(len(words), i + 6)
                    context = ' '.join(words[context_start:context_end])
                    
                    results.append({
                        'doc_id': doc_id,
                        'word_form': word,
                        'position': i,
                        'context': context
                    })
        
        return results
    
    def search_by_pos(self, corpus, pos_tag):
        """Поиск по части речи"""
        results = []
        
        for doc_id, text in enumerate(corpus):
            words = text.split()
            for i, word in enumerate(words):
                word_clean = re.sub(r'[^\w]', '', word)
                parsed = self.morph.parse(word_clean)[0]
                
                if pos_tag in str(parsed.tag):
                    results.append({
                        'doc_id': doc_id,
                        'word': word,
                        'pos': str(parsed.tag.POS),
                        'position': i
                    })
        
        return results
```

### 2.4 Поиск N-грамм и коллокаций

```python
from collections import Counter
from itertools import combinations

class NGramSearcher:
    def __init__(self, corpus):
        self.corpus = corpus
    
    def extract_ngrams(self, text, n):
        """Извлечение n-грамм из текста"""
        words = text.split()
        ngrams = []
        for i in range(len(words) - n + 1):
            ngrams.append(tuple(words[i:i+n]))
        return ngrams
    
    def find_frequent_ngrams(self, n, min_freq=5):
        """Поиск частотных n-грамм"""
        all_ngrams = []
        
        for text in self.corpus:
            all_ngrams.extend(self.extract_ngrams(text, n))
        
        ngram_counts = Counter(all_ngrams)
        frequent = {ngram: count for ngram, count 
                   in ngram_counts.items() if count >= min_freq}
        
        return sorted(frequent.items(), key=lambda x: x[1], reverse=True)
    
    def find_collocations(self, word, window=5):
        """Поиск коллокаций для заданного слова"""
        collocations = Counter()
        
        for text in self.corpus:
            words = text.split()
            for i, w in enumerate(words):
                if w.lower() == word.lower():
                    # Окно контекста
                    start = max(0, i - window)
                    end = min(len(words), i + window + 1)
                    context_words = words[start:i] + words[i+1:end]
                    
                    for context_word in context_words:
                        collocations[context_word.lower()] += 1
        
        return collocations.most_common(20)
```

---

## 3. ИНДЕКСИРОВАНИЕ КОРПУСА

Для эффективного поиска в больших корпусах используются различные методы индексирования.

### 3.1 Инвертированный индекс

```python
from collections import defaultdict

class InvertedIndex:
    def __init__(self):
        self.index = defaultdict(list)
        self.documents = []
    
    def add_document(self, doc_id, text):
        """Добавление документа в индекс"""
        self.documents.append(text)
        words = text.lower().split()
        
        for position, word in enumerate(words):
            # Очистка от пунктуации
            word_clean = re.sub(r'[^\w]', '', word)
            if word_clean:
                self.index[word_clean].append({
                    'doc_id': doc_id,
                    'position': position
                })
    
    def search(self, query):
        """Поиск по индексу"""
        query_clean = re.sub(r'[^\w]', '', query.lower())
        return self.index.get(query_clean, [])
    
    def boolean_search(self, query_terms, operator='AND'):
        """Булев поиск"""
        results_sets = []
        
        for term in query_terms:
            term_clean = re.sub(r'[^\w]', '', term.lower())
            doc_ids = {item['doc_id'] for item in self.index.get(term_clean, [])}
            results_sets.append(doc_ids)
        
        if operator == 'AND':
            if not results_sets:
                return set()
            result = results_sets[0]
            for s in results_sets[1:]:
                result = result.intersection(s)
        elif operator == 'OR':
            result = set()
            for s in results_sets:
                result = result.union(s)
        
        return sorted(list(result))
    
    def phrase_search(self, phrase):
        """Поиск фразы (слова должны идти подряд)"""
        words = [re.sub(r'[^\w]', '', w.lower()) for w in phrase.split()]
        
        if not words:
            return []
        
        # Находим документы, содержащие первое слово
        first_word_occurrences = self.index.get(words[0], [])
        results = []
        
        for occurrence in first_word_occurrences:
            doc_id = occurrence['doc_id']
            start_pos = occurrence['position']
            
            # Проверяем, идут ли следующие слова подряд
            match = True
            for i, word in enumerate(words[1:], 1):
                expected_occurrences = self.index.get(word, [])
                found = False
                
                for occ in expected_occurrences:
                    if occ['doc_id'] == doc_id and occ['position'] == start_pos + i:
                        found = True
                        break
                
                if not found:
                    match = False
                    break
            
            if match:
                results.append({
                    'doc_id': doc_id,
                    'start_position': start_pos
                })
        
        return results
```

### 3.2 TF-IDF и векторный поиск

```python
import math
from collections import Counter

class TFIDFSearcher:
    def __init__(self, corpus):
        self.corpus = corpus
        self.df = defaultdict(int)  # document frequency
        self.idf = {}
        self.tf_idf_matrix = []
        self._build_index()
    
    def _build_index(self):
        """Построение TF-IDF индекса"""
        N = len(self.corpus)
        
        # Подсчёт document frequency
        for doc in self.corpus:
            words = set(doc.lower().split())
            for word in words:
                self.df[word] += 1
        
        # Вычисление IDF
        for word, df in self.df.items():
            self.idf[word] = math.log(N / df)
        
        # Вычисление TF-IDF для каждого документа
        for doc in self.corpus:
            words = doc.lower().split()
            word_counts = Counter(words)
            doc_length = len(words)
            
            tf_idf = {}
            for word, count in word_counts.items():
                tf = count / doc_length
                tf_idf[word] = tf * self.idf.get(word, 0)
            
            self.tf_idf_matrix.append(tf_idf)
    
    def search(self, query, top_k=10):
        """Поиск наиболее релевантных документов"""
        query_words = query.lower().split()
        query_vec = {}
        
        # TF-IDF вектор запроса
        word_counts = Counter(query_words)
        for word, count in word_counts.items():
            tf = count / len(query_words)
            query_vec[word] = tf * self.idf.get(word, 0)
        
        # Вычисление косинусного сходства
        scores = []
        for doc_id, doc_vec in enumerate(self.tf_idf_matrix):
            score = self._cosine_similarity(query_vec, doc_vec)
            scores.append((doc_id, score))
        
        # Сортировка по релевантности
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:top_k]
    
    def _cosine_similarity(self, vec1, vec2):
        """Косинусное сходство двух векторов"""
        intersection = set(vec1.keys()) & set(vec2.keys())
        
        numerator = sum(vec1[word] * vec2[word] for word in intersection)
        
        sum1 = sum(val**2 for val in vec1.values())
        sum2 = sum(val**2 for val in vec2.values())
        denominator = math.sqrt(sum1) * math.sqrt(sum2)
        
        if denominator == 0:
            return 0
        return numerator / denominator
```

---

## 4. КОНКОРДАНС И KWIC

**KWIC (Key Word In Context)** — представление результатов поиска с контекстом.

```python
class ConcordanceBuilder:
    def __init__(self, corpus, context_size=50):
        self.corpus = corpus
        self.context_size = context_size
    
    def build_concordance(self, query):
        """Построение конкорданса для запроса"""
        concordance = []
        
        for doc_id, text in enumerate(self.corpus):
            pattern = re.compile(re.escape(query), re.IGNORECASE)
            
            for match in pattern.finditer(text):
                left_context_start = max(0, match.start() - self.context_size)
                right_context_end = min(len(text), match.end() + self.context_size)
                
                left_context = text[left_context_start:match.start()]
                keyword = text[match.start():match.end()]
                right_context = text[match.end():right_context_end]
                
                concordance.append({
                    'doc_id': doc_id,
                    'left': left_context,
                    'keyword': keyword,
                    'right': right_context
                })
        
        return concordance
    
    def display_kwic(self, concordance, max_width=80):
        """Отображение в формате KWIC"""
        for item in concordance:
            left = item['left'].rjust(max_width // 2)
            right = item['right'][:max_width // 2]
            print(f"{left} [{item['keyword']}] {right}")
```

---

## 5. ПРАКТИЧЕСКИЕ УПРАЖНЕНИЯ

### Упражнение 1: Базовый поиск и статистика
**Задание:** Напишите программу, которая:
1. Загружает корпус текстов (можете использовать несколько текстовых файлов)
2. Выполняет поиск заданного слова
3. Выводит частотность слова в каждом документе
4. Показывает общую статистику по корпусу (количество слов, уникальных слов)

```python
# Ваш код здесь
def corpus_statistics(corpus, search_word):
    # TODO: Реализовать функцию
    pass
```

### Упражнение 2: Поиск с регулярными выражениями
**Задание:** Создайте функцию, которая находит в корпусе:
1. Все email-адреса
2. Все даты в формате ДД.ММ.ГГГГ или ДД/ММ/ГГГГ
3. Все слова, начинающиеся с заглавной буквы (потенциальные имена собственные)

```python
# Шаблон для выполнения
def find_patterns(corpus):
    email_pattern = r''  # TODO: написать регулярное выражение
    date_pattern = r''   # TODO: написать регулярное выражение
    proper_names_pattern = r''  # TODO: написать регулярное выражение
    
    # TODO: Реализовать поиск
    pass
```

### Упражнение 3: Построение инвертированного индекса
**Задание:** Реализуйте класс InvertedIndex с дополнительной функциональностью:
1. Метод для поиска по префиксу (например, "прог*" найдёт "программа", "программирование" и т.д.)
2. Метод для поиска с учётом расстояния между словами (proximity search)
3. Метод для получения статистики по индексу (количество уникальных слов, средняя длина списка постингов)

```python
# Расширьте базовый класс InvertedIndex
class ExtendedInvertedIndex(InvertedIndex):
    def prefix_search(self, prefix):
        # TODO: Реализовать
        pass
    
    def proximity_search(self, word1, word2, max_distance=5):
        # TODO: Реализовать
        pass
    
    def get_statistics(self):
        # TODO: Реализовать
        pass
```

### Упражнение 4: Морфологический анализ и поиск
**Задание:** Используя библиотеку pymorphy2:
1. Найдите все существительные в родительном падеже
2. Найдите все глаголы прошедшего времени
3. Постройте частотный словарь лемм для корпуса

```python
import pymorphy2

morph = pymorphy2.MorphAnalyzer()

def morphological_analysis(corpus):
    # TODO: Реализовать анализ
    pass
```

### Упражнение 5: TF-IDF и ранжирование
**Задание:** 
1. Реализуйте поиск с использованием TF-IDF
2. Найдите топ-10 самых характерных слов для каждого документа в корпусе
3. Реализуйте функцию поиска похожих документов

```python
def find_characteristic_words(corpus, doc_id, top_n=10):
    # TODO: Найти слова с наибольшим TF-IDF для документа
    pass

def find_similar_documents(corpus, doc_id, top_n=5):
    # TODO: Найти похожие документы
    pass
```

---

## 6. ОПТИМИЗАЦИЯ И МАСШТАБИРУЕМОСТЬ

### 6.1 Работа с большими корпусами
- Использование генераторов вместо загрузки всего корпуса в память
- Индексирование на диске (SQLite, Whoosh, Elasticsearch)
- Параллельная обработка

```python
def process_large_corpus(corpus_path):
    """Пример работы с большим корпусом через генератор"""
    def document_generator():
        with open(corpus_path, 'r', encoding='utf-8') as f:
            for line in f:
                yield line.strip()
    
    # Обработка по одному документу
    for doc in document_generator():
        # Обработка документа
        pass
```

### 6.2 Использование специализированных библиотек

**NLTK для работы с корпусами:**
```python
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Удаление стоп-слов
stop_words = set(stopwords.words('russian'))
filtered_words = [w for w in word_tokenize(text) if w.lower() not in stop_words]
```

**Whoosh для полнотекстового поиска:**
```python
from whoosh.index import create_in
from whoosh.fields import Schema, TEXT, ID
from whoosh.qparser import QueryParser

# Создание схемы
schema = Schema(id=ID(stored=True), content=TEXT(stored=True))

# Создание индекса
# ix = create_in("indexdir", schema)
```

---

## 7. КОНТРОЛЬНЫЕ ВОПРОСЫ

**Вопрос 1:** Объясните разницу между поиском по словоформе и поиском по лемме. В каких случаях каждый из этих подходов предпочтительнее? Приведите примеры запросов, где принципиально важно использовать лемматизацию.

**Вопрос 2:** Что такое инвертированный индекс и как он ускоряет поиск в корпусе? Опишите его структуру данных и алгоритм построения. Какие операции поиска (булев поиск, фразовый поиск, поиск по префиксу) можно эффективно реализовать с его помощью?

**Вопрос 3:** Объясните, как работает метрика TF-IDF. Почему для ранжирования документов используется именно косинусное сходство векторов, а не, например, евклидово расстояние? Какие ограничения имеет подход на основе TF-IDF?

**Вопрос 4:** Что такое коллокации и какие статистические меры используются для их выявления в корпусе? Почему простая частотность совместной встречаемости двух слов недостаточна для определения устойчивых словосочетаний?

**Вопрос 5:** Опишите проблемы масштабируемости при работе с большими корпусами (сотни гигабайт текста). Какие подходы и технологии можно использовать для эффективной индексации и поиска в таких объёмах данных? Сравните подходы in-memory индексирования и индексирования на диске.

---

## РЕКОМЕНДУЕМАЯ ЛИТЕРАТУРА

1. Manning C.D., Raghavan P., Schütze H. Introduction to Information Retrieval. Cambridge University Press, 2008.
2. Jurafsky D., Martin J.H. Speech and Language Processing. 3rd edition draft, 2023.
3. Захаров В.П., Богданова С.Ю. Корпусная лингвистика: Учебник для студентов гуманитарных вузов. Иркутск, 2011.
4. Sinclair J. Corpus, Concordance, Collocation. Oxford University Press, 1991.
5. Bird S., Klein E., Loper E. Natural Language Processing with Python. O'Reilly, 2009.

---

## ГЛОССАРИЙ

- **Корпус** — систематизированное собрание текстов в электронном виде
- **Конкорданс** — список всех употреблений слова в контексте
- **KWIC** — представление ключевого слова в контексте (Key Word In Context)
- **Лемма** — словарная (начальная) форма слова
- **N-грамма** — последовательность из N элементов (слов, символов)
- **Коллокация** — устойчивое словосочетание
- **TF-IDF** — мера важности слова в документе относительно корпуса
- **Инвертированный индекс** — структура данных для быстрого поиска
- **Стоп-слова** — частотные служебные слова, обычно исключаемые из анализа