# Поиск в корпусе текста
### Технологии программирования | Компьютерная лингвистика

---

## Что такое корпус?

**Корпус** — это большая коллекция текстов в электронном виде, собранная для лингвистического анализа.

Примеры корпусов:
- **Общие** — Национальный корпус русского языка
- **Специализированные** — научные статьи, художественная литература
- **Параллельные** — для машинного перевода (текст + перевод)
- **Диахронические** — для изучения исторических изменений языка

---

## Уровень 1 — Простой поиск

Самый базовый вид поиска: проверяем, есть ли нужное слово в тексте.

```python
# Поиск точного совпадения
def simple_search(corpus, query):
    results = []
    for doc_id, text in enumerate(corpus):
        if query in text:
            results.append((doc_id, text))
    return results
```

```python
# Поиск без учёта регистра (находит и "Привет", и "привет")
def case_insensitive_search(corpus, query):
    query_lower = query.lower()
    results = []
    for doc_id, text in enumerate(corpus):
        if query_lower in text.lower():
            results.append((doc_id, text))
    return results
```

> **Проблема:** простой поиск находит только точную форму слова. Запрос `"идёт"` не найдёт `"шёл"` или `"пойдёт"`.

---

## Уровень 2 — Поиск с регулярными выражениями

Регулярные выражения позволяют искать по шаблону, а не по конкретной строке.

```python
import re

def regex_search(corpus, pattern):
    compiled_pattern = re.compile(pattern, re.IGNORECASE)
    results = []

    for doc_id, text in enumerate(corpus):
        for match in compiled_pattern.finditer(text):
            # Берём 50 символов контекста слева и справа
            context = text[max(0, match.start()-50) : match.end()+50]
            results.append({
                'doc_id':   doc_id,
                'match':    match.group(),
                'position': match.start(),
                'context':  context
            })
    return results
```

**Полезные шаблоны:**

| Что ищем | Паттерн |
|---|---|
| Слова с приставкой «пред-» | `r'\bпред\w+'` |
| Формы глагола «идти» | `r'\b(ид|шёл|шла|шли|пойд)\w*'` |
| Email-адреса | `r'[\w.-]+@[\w.-]+\.\w+'` |
| Даты ДД.ММ.ГГГГ | `r'\d{2}[./]\d{2}[./]\d{4}'` |
| Слова с заглавной буквы | `r'\b[А-ЯЁ][а-яё]+'` |

---

## Уровень 3 — Лингвистический поиск (с лемматизацией)

**Лемма** — это начальная (словарная) форма слова. Например, лемма для слов «идёт», «шли», «пойдут» — это «идти».

Библиотека `pymorphy2` позволяет работать с русской морфологией.

```python
import pymorphy2, re

class MorphologicalSearcher:
    def __init__(self):
        self.morph = pymorphy2.MorphAnalyzer()

    def lemmatize(self, word):
        """Возвращает начальную форму слова"""
        return self.morph.parse(word)[0].normal_form

    def search_by_lemma(self, corpus, lemma):
        """Находит все словоформы по одной лемме"""
        results = []
        for doc_id, text in enumerate(corpus):
            words = text.split()
            for i, word in enumerate(words):
                word_clean = re.sub(r'[^\w]', '', word)
                if self.lemmatize(word_clean) == lemma:
                    # Контекст: 5 слов до и после
                    context = ' '.join(words[max(0,i-5) : i+6])
                    results.append({'doc_id': doc_id, 'word_form': word, 'context': context})
        return results

    def search_by_pos(self, corpus, pos_tag):
        """Поиск по части речи: 'NOUN', 'VERB', 'ADJF' и т.д."""
        results = []
        for doc_id, text in enumerate(corpus):
            words = text.split()
            for i, word in enumerate(words):
                word_clean = re.sub(r'[^\w]', '', word)
                parsed = self.morph.parse(word_clean)[0]
                if pos_tag in str(parsed.tag):
                    results.append({'doc_id': doc_id, 'word': word, 'pos': str(parsed.tag.POS)})
        return results
```

---

## Уровень 4 — N-граммы и коллокации

**N-грамма** — последовательность из N слов. Биграммы (`n=2`): «красная шапочка», «горячий кофе».

**Коллокация** — устойчивое словосочетание, которое слова образуют чаще, чем случайно.

```python
from collections import Counter

class NGramSearcher:
    def __init__(self, corpus):
        self.corpus = corpus

    def extract_ngrams(self, text, n):
        words = text.split()
        return [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]

    def find_frequent_ngrams(self, n, min_freq=5):
        """Возвращает самые частые n-граммы"""
        all_ngrams = []
        for text in self.corpus:
            all_ngrams.extend(self.extract_ngrams(text, n))

        counts = Counter(all_ngrams)
        frequent = {ng: c for ng, c in counts.items() if c >= min_freq}
        return sorted(frequent.items(), key=lambda x: x[1], reverse=True)

    def find_collocations(self, word, window=5):
        """Слова, которые чаще всего встречаются рядом с заданным"""
        collocations = Counter()
        for text in self.corpus:
            words = text.split()
            for i, w in enumerate(words):
                if w.lower() == word.lower():
                    context = words[max(0,i-window):i] + words[i+1:i+window+1]
                    for cw in context:
                        collocations[cw.lower()] += 1
        return collocations.most_common(20)
```

---

## Уровень 5 — Инвертированный индекс

Для больших корпусов перебор каждого текста слишком медленный. Решение — **инвертированный индекс**: словарь вида `слово → список (документ, позиция)`.

```
"кот"  → [(doc_0, pos_3), (doc_2, pos_17), ...]
"мышь" → [(doc_0, pos_5), (doc_1, pos_1),  ...]
```

```python
from collections import defaultdict
import re

class InvertedIndex:
    def __init__(self):
        self.index = defaultdict(list)
        self.documents = []

    def add_document(self, doc_id, text):
        self.documents.append(text)
        for pos, word in enumerate(text.lower().split()):
            clean = re.sub(r'[^\w]', '', word)
            if clean:
                self.index[clean].append({'doc_id': doc_id, 'position': pos})

    def search(self, query):
        """Простой поиск по индексу"""
        return self.index.get(query.lower(), [])

    def boolean_search(self, terms, operator='AND'):
        """Булев поиск: AND (все слова) или OR (хотя бы одно)"""
        sets = [
            {item['doc_id'] for item in self.index.get(t.lower(), [])}
            for t in terms
        ]
        if operator == 'AND':
            return sorted(sets[0].intersection(*sets[1:]) if sets else set())
        else:  # OR
            result = set()
            for s in sets: result |= s
            return sorted(result)

    def phrase_search(self, phrase):
        """Поиск точной фразы — слова идут строго подряд"""
        words = [re.sub(r'[^\w]', '', w.lower()) for w in phrase.split()]
        if not words:
            return []

        candidates = self.index.get(words[0], [])
        results = []

        for occ in candidates:
            doc_id, start = occ['doc_id'], occ['position']
            if all(
                any(o['doc_id'] == doc_id and o['position'] == start + i
                    for o in self.index.get(w, []))
                for i, w in enumerate(words[1:], 1)
            ):
                results.append({'doc_id': doc_id, 'start_position': start})
        return results
```

---

## Уровень 6 — TF-IDF и ранжирование

**TF-IDF** показывает, насколько слово характерно для конкретного документа:

- **TF** (Term Frequency) — как часто слово встречается в документе
- **IDF** (Inverse Document Frequency) — насколько слово редкое во всём корпусе
- Формула: `TF-IDF = TF × log(N / DF)`

Чем выше TF-IDF, тем более «характерным» является слово для документа.

```python
import math
from collections import Counter, defaultdict

class TFIDFSearcher:
    def __init__(self, corpus):
        self.corpus = corpus
        self.idf = {}
        self.tf_idf_matrix = []
        self._build_index()

    def _build_index(self):
        N = len(self.corpus)
        df = defaultdict(int)

        for doc in self.corpus:
            for word in set(doc.lower().split()):
                df[word] += 1

        self.idf = {word: math.log(N / freq) for word, freq in df.items()}

        for doc in self.corpus:
            words = doc.lower().split()
            counts = Counter(words)
            self.tf_idf_matrix.append({
                word: (count / len(words)) * self.idf.get(word, 0)
                for word, count in counts.items()
            })

    def search(self, query, top_k=10):
        """Возвращает топ-K наиболее релевантных документов"""
        qwords = query.lower().split()
        qcounts = Counter(qwords)
        query_vec = {
            w: (c / len(qwords)) * self.idf.get(w, 0)
            for w, c in qcounts.items()
        }

        scores = [
            (doc_id, self._cosine_similarity(query_vec, doc_vec))
            for doc_id, doc_vec in enumerate(self.tf_idf_matrix)
        ]
        return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]

    def _cosine_similarity(self, v1, v2):
        common = set(v1) & set(v2)
        num = sum(v1[w] * v2[w] for w in common)
        den = math.sqrt(sum(x**2 for x in v1.values())) * \
              math.sqrt(sum(x**2 for x in v2.values()))
        return num / den if den else 0
```

> Для сравнения документов используется **косинусное сходство**, а не евклидово расстояние — потому что нам важно направление вектора (тематика), а не его длина (объём документа).

---

## Уровень 7 — Конкорданс и KWIC

**KWIC (Key Word In Context)** — стандартный способ показать результаты поиска: ключевое слово выровнено по центру, слева и справа — контекст.

```
         он вышел из дома и [пошёл] по улице вниз к ре
     она долго думала, но [пошла] за ним следом, не ог
```

```python
class ConcordanceBuilder:
    def __init__(self, corpus, context_size=50):
        self.corpus = corpus
        self.context_size = context_size

    def build_concordance(self, query):
        concordance = []
        pattern = re.compile(re.escape(query), re.IGNORECASE)

        for doc_id, text in enumerate(self.corpus):
            for match in pattern.finditer(text):
                concordance.append({
                    'doc_id':  doc_id,
                    'left':    text[max(0, match.start()-self.context_size) : match.start()],
                    'keyword': match.group(),
                    'right':   text[match.end() : match.end()+self.context_size]
                })
        return concordance

    def display_kwic(self, concordance, max_width=80):
        for item in concordance:
            left  = item['left'].rjust(max_width // 2)
            right = item['right'][:max_width // 2]
            print(f"{left} [{item['keyword']}] {right}")
```

---

## Работа с большими корпусами

При объёмах в сотни гигабайт нельзя загружать весь корпус в память. Используем **генераторы**:

```python
def process_large_corpus(corpus_path):
    def document_generator():
        with open(corpus_path, 'r', encoding='utf-8') as f:
            for line in f:
                yield line.strip()  # по одному документу за раз

    for doc in document_generator():
        pass  # обрабатываем каждый документ отдельно
```

**Специализированные инструменты:**

```python
# NLTK: удаление стоп-слов
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('russian'))
filtered = [w for w in word_tokenize(text) if w.lower() not in stop_words]

# Whoosh: полнотекстовый поиск на диске
from whoosh.fields import Schema, TEXT, ID
schema = Schema(id=ID(stored=True), content=TEXT(stored=True))
```

---

## Глоссарий

| Термин | Определение |
|---|---|
| **Корпус** | Систематизированное собрание текстов в электронном виде |
| **Конкорданс** | Список всех употреблений слова с контекстом |
| **KWIC** | Key Word In Context — ключевое слово в контексте |
| **Лемма** | Начальная (словарная) форма слова |
| **N-грамма** | Последовательность из N слов или символов |
| **Коллокация** | Устойчивое словосочетание |
| **TF-IDF** | Мера важности слова в документе относительно корпуса |
| **Инвертированный индекс** | Структура данных: слово → список документов и позиций |
| **Стоп-слова** | Частотные служебные слова, исключаемые из анализа |

---

## Рекомендуемая литература

1. Manning, Raghavan, Schütze — *Introduction to Information Retrieval* (2008)
2. Jurafsky, Martin — *Speech and Language Processing* (3rd ed., 2023)
3. Захаров, Богданова — *Корпусная лингвистика* (2011)
4. Bird, Klein, Loper — *Natural Language Processing with Python* (O'Reilly, 2009)